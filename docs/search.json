[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I’m Wang Huang Han, a dedicated data science master’s student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I’m engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data—a fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet’s connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Heart Disease Prediction\n\n\n\npython\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\nHuang Han Wang\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Plot\n\n\n\ncode\n\n\nanalysis\n\n\nplot\n\n\n\n\n\n\n\nMe\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html",
    "href": "posts/heart disease prediction/heart-disease.html",
    "title": "Heart Disease Prediction",
    "section": "",
    "text": "Using machine learning methods—KNN, Random Forest, and Logistic Regression—to predict heart disease diagnosis with patient data."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#description",
    "href": "posts/heart disease prediction/heart-disease.html#description",
    "title": "Heart Disease Prediction",
    "section": "Description",
    "text": "Description\nHeart disease is the world’s leading cause of death, taking around 17.9 million lives yearly, according to the World Health Organization (WHO). Early identification of high-risk individuals is crucial in preventing premature deaths.\nDiagnosing heart disease is complex, often relying on patient symptoms and examinations. Analyzing vast clinical data using data science helps us better understand and predict heart disease occurrence."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#dataset",
    "href": "posts/heart disease prediction/heart-disease.html#dataset",
    "title": "Heart Disease Prediction",
    "section": "Dataset",
    "text": "Dataset\n\nHeart Disease - UCI Machine Learning Repository\n\nFour of the processed files used in this project:\n\nprocessed.switzerland.data\nprocessed.cleveland.data\nprocessed.hungarian.data\nprocessed.va.data"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#project-objectives",
    "href": "posts/heart disease prediction/heart-disease.html#project-objectives",
    "title": "Heart Disease Prediction",
    "section": "Project objectives",
    "text": "Project objectives\n\nAnalyze the heart disease dataset to find common risk factors and patient groups.\nDevelop a heart disease prediction system using KNN, RandomForest, and Logistic Regression algorithms.\nAssess the performance of the created models.\n\n\n\nImport libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport palettable\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#data-loading",
    "href": "posts/heart disease prediction/heart-disease.html#data-loading",
    "title": "Heart Disease Prediction",
    "section": "Data loading",
    "text": "Data loading\n\nThere are 14 attributes in each dataset\nCombined 4 dataset into single dataset\nAll contain same the same columns (without any header, missing values shown as ?)\n\n\n\nCode\ncolumns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\",\n          \"slope\", \"ca\", \"thal\", \"disease\"]\nsdf = pd.read_csv(\"processed.switzerland.csv\", header=None, names=columns, na_values='?')\ncdf = pd.read_csv(\"processed.cleveland.csv\", header=None, names=columns, na_values='?')\nhdf = pd.read_csv(\"processed.hungarian.csv\", header=None, names=columns, na_values='?')\nvdf = pd.read_csv(\"processed.va.csv\", header=None, names=columns, na_values='?')\n\ndf = pd.concat([sdf, cdf, vdf, hdf], ignore_index=True)\ndf.disease = df['disease'].apply(lambda x: 1 if x &gt; 0 else 0)\ndf\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ndisease\n\n\n\n\n0\n32.0\n1.0\n1.0\n95.0\n0.0\nNaN\n0.0\n127.0\n0.0\n0.7\n1.0\nNaN\nNaN\n1\n\n\n1\n34.0\n1.0\n4.0\n115.0\n0.0\nNaN\nNaN\n154.0\n0.0\n0.2\n1.0\nNaN\nNaN\n1\n\n\n2\n35.0\n1.0\n4.0\nNaN\n0.0\nNaN\n0.0\n130.0\n1.0\nNaN\nNaN\nNaN\n7.0\n1\n\n\n3\n36.0\n1.0\n4.0\n110.0\n0.0\nNaN\n0.0\n125.0\n1.0\n1.0\n2.0\nNaN\n6.0\n1\n\n\n4\n38.0\n0.0\n4.0\n105.0\n0.0\nNaN\n0.0\n166.0\n0.0\n2.8\n1.0\nNaN\nNaN\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n915\n52.0\n1.0\n4.0\n160.0\n331.0\n0.0\n0.0\n94.0\n1.0\n2.5\nNaN\nNaN\nNaN\n1\n\n\n916\n54.0\n0.0\n3.0\n130.0\n294.0\n0.0\n1.0\n100.0\n1.0\n0.0\n2.0\nNaN\nNaN\n1\n\n\n917\n56.0\n1.0\n4.0\n155.0\n342.0\n1.0\n0.0\n150.0\n1.0\n3.0\n2.0\nNaN\nNaN\n1\n\n\n918\n58.0\n0.0\n2.0\n180.0\n393.0\n0.0\n0.0\n110.0\n1.0\n1.0\n2.0\nNaN\n7.0\n1\n\n\n919\n65.0\n1.0\n4.0\n130.0\n275.0\n0.0\n1.0\n115.0\n1.0\n1.0\n2.0\nNaN\nNaN\n1\n\n\n\n\n920 rows × 14 columns\n\n\n\n\nAttributes\nNumerical attributes\n\nage - age in years, numerical\ntrestbps - resting blood pressure (in mm Hg on admission to the hospital)\nchol - cholesterol in mg/dl\nthalach - maximum heart rate achieved\noldpeak - ST depression induced by exercise relative to rest. ‘ST’ relates to the positions on the electrocardiographic (ECG) plot.\nca - number of major vessels (0-3) colored by flouroscopy. Fluoroscopy is one of the most popular non-invasive coronary artery disease diagnosis. It enables the doctor to see the flow of blood through the coronary arteries in order to evaluate the presence of arterial blockages.\n\nCategorical attributes\n\nsex- sex\n\n1 = male\n0 = female\n\ncp- chest pain type\n\n1 = typical angina\n2 = atypical angina\n3 = non-anginal pain\n4 = asymptomatic\n\nfbs - fasting blood sugar &gt; 120 mg/dl\n\n1 = true\n0 = false\n\nrestecg - resting electrocardiographic (ECG) results\n\n0 = normal\n1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)\n2 = showing probable or definite left ventricular hypertrophy by Estes’ criteria\n\nexang - exercise induced angina. Angina is a type of chest pain caused by reduced blood flow to the heart.\n\n1 = yes\n0 = no\n\nslope - the slope of the peak exercise ST segment. (ECG)\n\n1 = upsloping\n2 = flat\n3 = downsloping\n\nthal - A blood disorder called thalassemia\n\n3 = normal blood flow\n6 = fixed defect (no blood flow in some part of the heart)\n7 = reversable defect (a blood flow is observed but it is not normal)\n\ndisease - refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. In this single dataset, ‘0’ signifies the absence of heart disease, while ‘1’ indicates diagnosed heart disease."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#exploratory-data-analysis-eda",
    "href": "posts/heart disease prediction/heart-disease.html#exploratory-data-analysis-eda",
    "title": "Heart Disease Prediction",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\nNumeric data summary\n\n\nCode\n# Summary statistics for numeric data\nnumeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n\nnumeric_summary = df[numeric_cols].describe().transpose()\nprint(\"Summary Statistics for Numeric Columns:\")\nprint(numeric_summary)\n\n# Distribution plots for numeric attributes\nplt.figure(figsize=(12, 8))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.histplot(df[col].dropna(), kde=True)\n    plt.title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n\n\nSummary Statistics for Numeric Columns:\n          count        mean         std   min    25%    50%    75%    max\nage       920.0   53.510870    9.424685  28.0   47.0   54.0   60.0   77.0\ntrestbps  861.0  132.132404   19.066070   0.0  120.0  130.0  140.0  200.0\nchol      890.0  199.130337  110.780810   0.0  175.0  223.0  268.0  603.0\nthalach   865.0  137.545665   25.926276  60.0  120.0  140.0  157.0  202.0\noldpeak   858.0    0.878788    1.091226  -2.6    0.0    0.5    1.5    6.2\nca        309.0    0.676375    0.935653   0.0    0.0    0.0    1.0    3.0\n\n\n\n\n\n\nAge: The average age in the dataset is approximately 53 years, with most individuals falling between 47 and 60 years old. The youngest person is 28 years old, and the oldest is 77 years old.\nResting Blood Pressure (trestbps): The average resting blood pressure is around 132 mm Hg, with readings typically ranging from 120 to 140 mm Hg. However, there seem to be some unusually low values (minimum at 0) that might need further investigation.\nCholesterol (chol): The average cholesterol level is about 199 mg/dl, with most values spanning between 175 and 268 mg/dl. There are also some entries with cholesterol levels recorded as 0, which might need verification.\nMaximum Heart Rate Achieved (thalach): On average, the maximum heart rate achieved is approximately 138 bpm, with the majority falling between 120 and 157 bpm.\nST Depression (oldpeak): The ST depression induced by exercise relative to rest averages around 0.88. The values range from -2.6 to 6.2.\nNumber of Major Vessels (ca): There are fewer data points available for the number of major vessels. On average, it appears that the dataset has about 0.68 major vessels colored by fluoroscopy, with values ranging from 0 to 3.\n\nGenerate a heatmap to display the correlations between different numeric attributes in heart disease dataset, helping to identify potential relationships or dependencies between these features.\n\n\nCode\n# Compute the correlation matrix\ncorr_matrix = df[numeric_cols].corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)\nplt.title('Correlation Heatmap of Numeric Features')\nplt.show()\n\n\n\n\n\n\n\nCategorical data summary\n\n\nCode\n# Categorical data summary with count plots\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'disease']\n\nplt.figure(figsize=(12, 10))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 3, i)\n    sns.countplot(data=df, x=col)\n    plt.title(f'Count Plot of {col}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCounts for each categorical attribute\n#categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'disease']\n\n#for col in categorical_cols:\n#    counts = df[col].value_counts()\n#    print(f\"Value counts for {col}:\")\n#    print(counts)\n#    print()\n\n\n\nSex: There are 726 instances of the value 1.0 (male) and 194 instances of the value 0.0 (female) in the ‘sex’ column.\nChest Pain Type (cp): 496 instances of type 4 (asymptomatic), 204 instances of type 3 (non-anginal pain), 174 instances of type 2 (atypical angina), and 46 instances of type 1 (typical angina).\nFasting Blood Sugar (fbs): 692 instances with a value of 0.0 (fasting blood sugar &lt;= 120 mg/dl) and 138 instances with a value of 1.0 (fasting blood sugar &gt; 120 mg/dl).\nResting Electrocardiographic Results (restecg): 551 instances with a value of 0.0 (normal), 188 instances with a value of 2.0 (showing probable or definite left ventricular hypertrophy by Estes’ criteria), and 179 instances with a value of 1.0 (having ST-T wave abnormality).\nExercise Induced Angina (exang): 528 instances with a value of 0.0 (no exercise-induced angina) and 337 instances with a value of 1.0 (presence of exercise-induced angina).\nSlope of Peak Exercise ST Segment (slope): 345 instances with a value of 2.0 (a flat slope), 203 instances with a value of 1.0 (an upsloping slope), and 63 instances with a value of 3.0 (a downsloping slope).\nThalassemia (thal): 196 instances with a value of 3.0 (normal blood flow), 192 instances with a value of 7.0 (reversible defect), and 46 instances with a value of 6.0 (fixed defect).\nPresence of Heart Disease (disease): 509 instances with a value of 1 (presence of heart disease) and 411 instances with a value of 0 (absence of heart disease).\n\n\n\nDifferent risk factors affect heart disease\n\n\nCode\n# Display boxplots for each numerical attribute and presence of heart disease \nplt.figure(figsize=(12, 10))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(x='disease', y=col, data=df)\n    plt.title(f'{col.capitalize()} vs Disease')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\n# Grouping by 'disease' and describing numeric attributes\nnumeric_summary = df.groupby('disease')[numeric_cols].describe().transpose()\n\n# Renaming the columns for clarity\nnumeric_summary.columns = ['non-heart disease (disease=0)', 'heart disease (disease=1)']\n\n# Displaying the summary table\nprint(numeric_summary)\n\n\n                non-heart disease (disease=0)  heart disease (disease=1)\nage      count                     411.000000                 509.000000\n         mean                       50.547445                  55.903733\n         std                         9.433700                   8.718959\n         min                        28.000000                  31.000000\n         25%                        43.000000                  51.000000\n         50%                        51.000000                  57.000000\n         75%                        57.000000                  62.000000\n         max                        76.000000                  77.000000\ntrestbps count                     391.000000                 470.000000\n         mean                      129.913043                 133.978723\n         std                        16.869867                  20.552278\n         min                        80.000000                   0.000000\n         25%                       120.000000                 120.000000\n         50%                       130.000000                 130.000000\n         75%                       140.000000                 145.000000\n         max                       190.000000                 200.000000\nchol     count                     392.000000                 498.000000\n         mean                      227.905612                 176.479920\n         std                        75.832760                 127.517611\n         min                         0.000000                   0.000000\n         25%                       199.000000                   0.000000\n         50%                       228.000000                 218.000000\n         75%                       269.000000                 267.750000\n         max                       564.000000                 603.000000\nthalach  count                     391.000000                 474.000000\n         mean                      148.800512                 128.261603\n         std                        23.608692                  24.024193\n         min                        69.000000                  60.000000\n         25%                       134.500000                 112.000000\n         50%                       151.000000                 128.000000\n         75%                       167.500000                 145.000000\n         max                       202.000000                 195.000000\noldpeak  count                     390.000000                 468.000000\n         mean                        0.418205                   1.262607\n         std                         0.715636                   1.197424\n         min                        -1.100000                  -2.600000\n         25%                         0.000000                   0.000000\n         50%                         0.000000                   1.050000\n         75%                         0.800000                   2.000000\n         max                         4.200000                   6.200000\nca       count                     165.000000                 144.000000\n         mean                        0.278788                   1.131944\n         std                         0.640006                   1.012140\n         min                         0.000000                   0.000000\n         25%                         0.000000                   0.000000\n         50%                         0.000000                   1.000000\n         75%                         0.000000                   2.000000\n         max                         3.000000                   3.000000\n\n\nFrom numerical attributes:\n\nAge: Individuals with heart disease tend to be older on average compared to those without. The median age for individuals with heart disease (57 years) is higher than for those without (51 years).\nResting Blood Pressure (trestbps): While the mean resting blood pressure appears slightly higher for individuals with heart disease, there’s overlap in the interquartile ranges, suggesting variability. However, there seem to be some unusual zero values for blood pressure in the heart disease group that might need further examination.\nCholesterol (chol): There’s a notable difference in cholesterol levels between the two groups. Individuals without heart disease have higher median cholesterol levels (228 mg/dl) compared to those with heart disease (218 mg/dl). However, there are zero values present in both groups that might need clarification.\nMaximum Heart Rate Achieved (thalach): Those without heart disease generally achieve higher maximum heart rates compared to those with heart disease. The median maximum heart rate for individuals without heart disease (151 bpm) is higher than for those with heart disease (128 bpm).\nST Depression (oldpeak): The magnitude of ST depression induced by exercise relative to rest appears significantly higher in individuals with heart disease. The median ST depression for those with heart disease (1.05) is notably greater compared to those without heart disease (0).\nNumber of Major Vessels (ca): Individuals with heart disease tend to have a higher number of major vessels colored by fluoroscopy. The median number of major vessels for those with heart disease (1) is higher than for those without heart disease (0).\nOutliers: In ‘trestbps’ and ‘chol’ columns, ‘trestbps’ displays an outlier with a minimum value of 0, while ‘chol’ has outliers represented by zero values in both categories, which may require further investigation due to their deviation from expected physiological ranges.\n\n\n\nCode\n# Generate stacked barcharts for each categorical value compared to heart disease\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n\nplt.figure(figsize=(15, 12))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 3, i)\n    ct = pd.crosstab(df[col], df['disease'], normalize='index') * 100\n    plot = ct.plot(kind='bar', stacked=True, ax=plt.gca())\n    plt.title(f'{col.capitalize()} vs Disease')\n    plt.ylabel('Percentage')\n    plt.xlabel(col.capitalize())\n    plt.legend(title='Disease', labels=['No Disease', 'Heart Disease'])\n    plt.xticks(rotation=0)\n    \n    for p in plot.patches:\n        width, height = p.get_width(), p.get_height()\n        x, y = p.get_xy() \n        plt.text(x + width / 2, \n                 y + height / 2, \n                 f'{height:.2f}%', \n                 horizontalalignment='center', \n                 verticalalignment='center')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFrom categorical attributes:\n\nSex: Females (sex=0) show a lower percentage of heart disease (25.77%) compared to males (sex=1) with a higher percentage (63.22%).\nChest Pain Type (Cp): Asymptomatic chest pain type (cp=4) displays a significantly higher percentage (79.03%) of heart disease, while typical angina (cp=1) has a lower percentage (43.48%).\nFasting Blood Sugar (Fbs): Individuals with fasting blood sugar greater than 120 mg/dl (fbs=1) tend to have a notably higher percentage (68.12%) of heart disease compared to those with lower fasting blood sugar.\nExercise Induced Angina (Exang): Individuals experiencing exercise-induced angina (exang=1) demonstrate a notably higher percentage (83.68%) of heart disease compared to those without it.\nThalassemia (Thal): Reversible defect thalassemia (thal=7) shows a higher percentage (80.21%) of heart disease compared to other types of thalassemia.\n\n\n\nSummary of EDA\nInsights from the Exploratory Data Analysis:\n\nAge Factor: Individuals diagnosed with heart disease tend to be older than those without the condition, suggesting age as a contributing risk factor for heart disease.\nChest Pain Types: Asymptomatic chest pain (cp=4) presents a notably higher prevalence of heart disease, while typical angina (cp=1) exhibits a lower association with heart conditions.\nCholesterol and Heart Disease: While cholesterol levels vary, individuals without heart disease often exhibit higher median cholesterol levels than those with the condition, although zero values in both groups require further validation.\nExercise-Induced Angina: The presence of exercise-induced angina (exang=1) demonstrates a significantly higher likelihood of heart disease compared to its absence (exang=0).\nGender Disparity: Males (sex=1) tend to show a higher percentage of heart disease cases (63.22%) compared to females (sex=0) with a lower prevalence (25.77%).\nFasting Blood Sugar (FBS): Elevated fasting blood sugar (&gt;120 mg/dl) correlates with a higher percentage of heart disease instances (68.12%) compared to lower levels.\nResting Blood Pressure and Heart Disease: Resting blood pressure, though variable, shows a trend toward higher values for heart disease cases, though outliers (0 values) necessitate further investigation."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#data-preparation",
    "href": "posts/heart disease prediction/heart-disease.html#data-preparation",
    "title": "Heart Disease Prediction",
    "section": "Data preparation",
    "text": "Data preparation\n\nFind incorrect values\n\nCholesterol (chol) contains lots of zero values.\nST Depression (oldpeak) gets some negetive values.\nResting Blood Pressure (trestbps) has one zero value.\n\nCount and plot those incorrect (missing) values.\n\n\nCode\n# Create a copy of the dataframe to handle missing values\ndf_missing = df.copy()\n\n# Replace incorrect values with NaN in specific columns\ndf_missing['chol'] = df['chol'].replace({0: np.nan})\ndf_missing['trestbps'] = df['trestbps'].replace({0: np.nan})\ndf_missing.loc[df['oldpeak'] &lt; 0, 'oldpeak'] = np.nan\n\n# Calculate missing value counts and percentages\nna_values_percent = df_missing.isna().sum().sort_values(ascending=False) \\\n    .apply(lambda x: (x, round(x / len(df_missing) * 100, 2)))\n\n# Plotting the missing value percentages\nna_values_percent.apply(lambda x: x[1]).plot.bar(title='Percentage of Missing Values in each Column')\nplt.xlabel('Columns')\nplt.ylabel('Percentage of Missing Values')\n\n# Annotating the bars with values\nfor i, val in enumerate(na_values_percent.apply(lambda x: x[1])):\n    plt.text(i, val + 1, f\"{val}%\", ha='center', va='bottom')\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Display the count and percentage of missing/incorrect values\nprint(na_values_percent)\n\n\n\n\n\nca          (611, 66.41)\nthal        (486, 52.83)\nslope       (309, 33.59)\nchol        (202, 21.96)\nfbs           (90, 9.78)\noldpeak       (74, 8.04)\ntrestbps      (60, 6.52)\nthalach       (55, 5.98)\nexang         (55, 5.98)\nrestecg        (2, 0.22)\ncp              (0, 0.0)\nsex             (0, 0.0)\nage             (0, 0.0)\ndisease         (0, 0.0)\ndtype: object\n\n\n\n\nData cleaning\nSet incorrect values to NaN and find out duplicate values to remove.\n\n\nCode\n# Create a copy of the dataframe to handle missing values and duplicates\ndf_clean = df.copy()\n\n# Replace incorrect values with NaN in specific columns\ndf_clean['chol'] = df['chol'].replace({0: np.nan})\ndf_clean['trestbps'] = df['trestbps'].replace({0: np.nan})\ndf_clean.loc[df['oldpeak'] &lt; 0, 'oldpeak'] = np.nan\n\n# Display the count of missing values before cleaning\nprint(\"Missing Values Before Cleaning:\")\nprint(df.isnull().sum())\n\n# Remove duplicate rows\nduplicate_rows = df_clean[df_clean.duplicated()]\nprint(\"\\nDuplicate Rows:\")\nprint(duplicate_rows)\n\n# Drop duplicate rows\ndf_clean.drop_duplicates(inplace=True)\n\n# Display the count of missing values after cleaning\nprint(\"\\nMissing Values After Cleaning:\")\nprint(df_clean.isnull().sum())\n\n\nMissing Values Before Cleaning:\nage           0\nsex           0\ncp            0\ntrestbps     59\nchol         30\nfbs          90\nrestecg       2\nthalach      55\nexang        55\noldpeak      62\nslope       309\nca          611\nthal        486\ndisease       0\ndtype: int64\n\nDuplicate Rows:\n      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n613  58.0  1.0  3.0     150.0  219.0  0.0      1.0    118.0    1.0      0.0   \n728  49.0  0.0  2.0     110.0    NaN  0.0      0.0    160.0    0.0      0.0   \n\n     slope  ca  thal  disease  \n613    NaN NaN   NaN        1  \n728    NaN NaN   NaN        0  \n\nMissing Values After Cleaning:\nage           0\nsex           0\ncp            0\ntrestbps     60\nchol        201\nfbs          90\nrestecg       2\nthalach      55\nexang        55\noldpeak      74\nslope       307\nca          609\nthal        484\ndisease       0\ndtype: int64"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#feature-engineering",
    "href": "posts/heart disease prediction/heart-disease.html#feature-engineering",
    "title": "Heart Disease Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nData imputation\n\n\nCode\n# Columns for mode imputation\nmode_cols = ['restecg', 'exang', 'fbs', 'slope', 'thal']\n\n# Columns for median imputation\nmedian_cols = ['oldpeak', 'trestbps', 'thalach', 'chol', 'ca']\n\n# Impute missing values with mode for mode_cols\nmode_imputer = SimpleImputer(strategy='most_frequent')\ndf_clean[mode_cols] = mode_imputer.fit_transform(df_clean[mode_cols])\n\n# Impute missing values with median for median_cols\nmedian_imputer = SimpleImputer(strategy='median')\ndf_clean[median_cols] = median_imputer.fit_transform(df_clean[median_cols])\n\n# Verify the imputed values\nprint(\"Missing Values After Imputation:\")\nprint(df_clean.isnull().sum())\n\n\nMissing Values After Imputation:\nage         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ndisease     0\ndtype: int64\n\n\n\n\nCode\n# Check the number of rows after data imputation\nnum_rows_after_imputation = df_clean.shape[0]\nprint(f\"Number of rows after data imputation: {num_rows_after_imputation}\")\n\n\nNumber of rows after data imputation: 918\n\n\nBefore doing data split, the total number of rows are 918 since only removed 2 duplicate rows.\n\n\nOne-hot encoding\nIt’s important to apply One-Hot Encoding before splitting data into training and testing sets. Doing this ensures that each set is independently encoded, preventing any mixing of information between them. This separation is important because it helps the model learn and make predictions correctly without being influenced by how the data was divided.\n\n\nCode\n# Categorical columns for one-hot encoding\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n\n# Apply one-hot encoding\ndf_encoded = pd.get_dummies(df_clean, columns=categorical_cols)\n\n# Display the first few rows to verify the encoding\nprint(df_encoded.head())\n\n\n    age  trestbps   chol  thalach  oldpeak   ca  disease  sex_0.0  sex_1.0  \\\n0  32.0      95.0  240.0    127.0      0.7  0.0        1    False     True   \n1  34.0     115.0  240.0    154.0      0.2  0.0        1    False     True   \n2  35.0     130.0  240.0    130.0      0.5  0.0        1    False     True   \n3  36.0     110.0  240.0    125.0      1.0  0.0        1    False     True   \n4  38.0     105.0  240.0    166.0      2.8  0.0        1     True    False   \n\n   cp_1.0  ...  restecg_1.0  restecg_2.0  exang_0.0  exang_1.0  slope_1.0  \\\n0    True  ...        False        False       True      False       True   \n1   False  ...        False        False       True      False       True   \n2   False  ...        False        False      False       True      False   \n3   False  ...        False        False      False       True      False   \n4   False  ...        False        False       True      False       True   \n\n   slope_2.0  slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n0      False      False      True     False     False  \n1      False      False      True     False     False  \n2       True      False     False     False      True  \n3       True      False     False      True     False  \n4      False      False      True     False     False  \n\n[5 rows x 26 columns]\n\n\n\n\nFeature scaling & Split dataset\nNormalization / Standardization While ML algorithms like Random Forest, linear regression, logistic regression, and neural networks don’t require scaling, it’s crucial for Distance algorithms such as KNN, K-means, and SVM.\nTo accommodate this, I’ll split the dataset: one portion for KNN and logistic regression, where normalization is employed, and another for RF modeling, where scaling isn’t necessary.\n70% for Train & 30% for Test\n\n\nCode\n# Select numerical attributes for scaling\nnumerical_cols = ['age', 'oldpeak', 'chol', 'thalach', 'trestbps']\n\n# Extract features and target variable\nX = df_encoded.drop('disease', axis=1)\ny = df_encoded['disease']\n\n# Splitting the dataset into train and test sets (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Separate the numerical columns for scaling\nX_train_numeric = X_train[numerical_cols]\nX_test_numeric = X_test[numerical_cols]\n\n# Feature scaling for numerical attributes for KNN and Logistic Regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_numeric)\nX_test_scaled = scaler.transform(X_test_numeric)\n\n# Replace the scaled numerical columns in the train and test sets for KNN and Logistic Regression\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_cols, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_cols, index=X_test.index)\n\nX_train_knn_logreg = X_train.copy()\nX_test_knn_logreg = X_test.copy()\n\nX_train_knn_logreg[numerical_cols] = X_train_scaled\nX_test_knn_logreg[numerical_cols] = X_test_scaled"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#modeling",
    "href": "posts/heart disease prediction/heart-disease.html#modeling",
    "title": "Heart Disease Prediction",
    "section": "Modeling",
    "text": "Modeling\n\nK-Nearest Neighbors (KNN)\n\n\nCode\nknn = KNeighborsClassifier()\nknn.fit(X_train_knn_logreg, y_train)\ny_pred_knn = knn.predict(X_test_knn_logreg)\n\n\n/Users/harrywang/.virtualenvs/r-reticulate/lib/python3.9/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.9 is maybe too old for this OS.\n  warnings.warn(\n\n\n\n\nLogistic Regression\n\n\nCode\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_knn_logreg, y_train)\ny_pred_log_reg = log_reg.predict(X_test_knn_logreg)\n\n\n\n\nRandom Forest (data used without scaling)\n\n\nCode\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#evaluation-models-performance",
    "href": "posts/heart disease prediction/heart-disease.html#evaluation-models-performance",
    "title": "Heart Disease Prediction",
    "section": "Evaluation models performance",
    "text": "Evaluation models performance\n\n\nCode\nmodels = {\n    \"KNN\": (y_test, y_pred_knn),\n    \"Logistic Regression\": (y_test, y_pred_log_reg),\n    \"Random Forest\": (y_test, y_pred_rf)\n}\n\nfor model_name, (true, pred) in models.items():\n    accuracy = accuracy_score(true, pred)\n    precision = precision_score(true, pred)\n    recall = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    auc = roc_auc_score(true, pred)\n    \n    print(f\"Metrics for {model_name}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"AUC Score: {auc:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(true, pred)\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\n\")\n\n\nMetrics for KNN:\nAccuracy: 0.8043\nPrecision: 0.7829\nRecall: 0.8954\nF1 Score: 0.8354\nAUC Score: 0.7932\nConfusion Matrix:\n[[ 85  38]\n [ 16 137]]\n\n\nMetrics for Logistic Regression:\nAccuracy: 0.7935\nPrecision: 0.7824\nRecall: 0.8693\nF1 Score: 0.8235\nAUC Score: 0.7842\nConfusion Matrix:\n[[ 86  37]\n [ 20 133]]\n\n\nMetrics for Random Forest:\nAccuracy: 0.8261\nPrecision: 0.8107\nRecall: 0.8954\nF1 Score: 0.8509\nAUC Score: 0.8176\nConfusion Matrix:\n[[ 91  32]\n [ 16 137]]\n\n\n\n\n\n\nCode\n# Define a function to plot confusion matrix\ndef plot_confusion(y_true, y_pred, title):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n# Assuming you have predictions for each model (y_pred_knn, y_pred_logreg, y_pred_rf) and y_test available\n\n# Plot confusion matrix for KNN\nplot_confusion(y_test, y_pred_knn, 'Confusion Matrix - KNN')\n\n# Plot confusion matrix for Logistic Regression\nplot_confusion(y_test, y_pred_log_reg, 'Confusion Matrix - Logistic Regression')\n\n# Plot confusion matrix for Random Forest\nplot_confusion(y_test, y_pred_rf, 'Confusion Matrix - Random Forest')"
  },
  {
    "objectID": "posts/post with plot/index.html",
    "href": "posts/post with plot/index.html",
    "title": "Post With Plot",
    "section": "",
    "text": "This is a post with plot.\n\n\nCode\nplot(mtcars$mpg)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I’m Wang Huang Han, a dedicated data science master’s student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I’m engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data—a fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet’s connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I’m Wang Huang Han, a dedicated data science master’s student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I’m engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data—a fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet’s connect and explore the possibilities of collaboration and innovation together!"
  }
]