[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Fascinating Malaysia Biodiversity - Research Motivation\n\n\n\n\n\n\n\nmalaysia\n\n\nbiodiversity\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\nHuang Han Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I’m Wang Huang Han, a dedicated data science master’s student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I’m engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data—a fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet’s connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I’m Wang Huang Han, a dedicated data science master’s student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I’m engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data—a fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet’s connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I’m Wang Huang Han, a dedicated data science master’s student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I’m engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data—a fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet’s connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html",
    "href": "posts/heart disease prediction/heart-disease.html",
    "title": "Heart Disease Prediction",
    "section": "",
    "text": "Using machine learning methods—KNN, Random Forest, and Logistic Regression—to predict heart disease diagnosis with patient data."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#description",
    "href": "posts/heart disease prediction/heart-disease.html#description",
    "title": "Heart Disease Prediction",
    "section": "Description",
    "text": "Description\nHeart disease is the world’s leading cause of death, taking around 17.9 million lives yearly, according to the World Health Organization (WHO). Early identification of high-risk individuals is crucial in preventing premature deaths.\nDiagnosing heart disease is complex, often relying on patient symptoms and examinations. Analyzing vast clinical data using data science helps us better understand and predict heart disease occurrence."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#dataset",
    "href": "posts/heart disease prediction/heart-disease.html#dataset",
    "title": "Heart Disease Prediction",
    "section": "Dataset",
    "text": "Dataset\n\nHeart Disease - UCI Machine Learning Repository\n\nFour of the processed files used in this project:\n\nprocessed.switzerland.data\nprocessed.cleveland.data\nprocessed.hungarian.data\nprocessed.va.data"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#project-objectives",
    "href": "posts/heart disease prediction/heart-disease.html#project-objectives",
    "title": "Heart Disease Prediction",
    "section": "Project objectives",
    "text": "Project objectives\n\nAnalyze the heart disease dataset to find common risk factors and patient groups.\nDevelop a heart disease prediction system using KNN, RandomForest, and Logistic Regression algorithms.\nAssess the performance of the created models.\n\n\n\nImport libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#data-loading",
    "href": "posts/heart disease prediction/heart-disease.html#data-loading",
    "title": "Heart Disease Prediction",
    "section": "Data loading",
    "text": "Data loading\n\nThere are 14 attributes in each dataset\nCombined 4 dataset into single dataset\nAll contain same the same columns (without any header, missing values shown as ?)\n\n\n\nCode\ncolumns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\",\n          \"slope\", \"ca\", \"thal\", \"disease\"]\nsdf = pd.read_csv(\"processed.switzerland.csv\", header=None, names=columns, na_values='?')\ncdf = pd.read_csv(\"processed.cleveland.csv\", header=None, names=columns, na_values='?')\nhdf = pd.read_csv(\"processed.hungarian.csv\", header=None, names=columns, na_values='?')\nvdf = pd.read_csv(\"processed.va.csv\", header=None, names=columns, na_values='?')\n\ndf = pd.concat([sdf, cdf, vdf, hdf], ignore_index=True)\ndf.disease = df['disease'].apply(lambda x: 1 if x &gt; 0 else 0)\ndf\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ndisease\n\n\n\n\n0\n32.0\n1.0\n1.0\n95.0\n0.0\nNaN\n0.0\n127.0\n0.0\n0.7\n1.0\nNaN\nNaN\n1\n\n\n1\n34.0\n1.0\n4.0\n115.0\n0.0\nNaN\nNaN\n154.0\n0.0\n0.2\n1.0\nNaN\nNaN\n1\n\n\n2\n35.0\n1.0\n4.0\nNaN\n0.0\nNaN\n0.0\n130.0\n1.0\nNaN\nNaN\nNaN\n7.0\n1\n\n\n3\n36.0\n1.0\n4.0\n110.0\n0.0\nNaN\n0.0\n125.0\n1.0\n1.0\n2.0\nNaN\n6.0\n1\n\n\n4\n38.0\n0.0\n4.0\n105.0\n0.0\nNaN\n0.0\n166.0\n0.0\n2.8\n1.0\nNaN\nNaN\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n915\n52.0\n1.0\n4.0\n160.0\n331.0\n0.0\n0.0\n94.0\n1.0\n2.5\nNaN\nNaN\nNaN\n1\n\n\n916\n54.0\n0.0\n3.0\n130.0\n294.0\n0.0\n1.0\n100.0\n1.0\n0.0\n2.0\nNaN\nNaN\n1\n\n\n917\n56.0\n1.0\n4.0\n155.0\n342.0\n1.0\n0.0\n150.0\n1.0\n3.0\n2.0\nNaN\nNaN\n1\n\n\n918\n58.0\n0.0\n2.0\n180.0\n393.0\n0.0\n0.0\n110.0\n1.0\n1.0\n2.0\nNaN\n7.0\n1\n\n\n919\n65.0\n1.0\n4.0\n130.0\n275.0\n0.0\n1.0\n115.0\n1.0\n1.0\n2.0\nNaN\nNaN\n1\n\n\n\n\n920 rows × 14 columns\n\n\n\n\nAttributes\nNumerical attributes\n\nage - age in years, numerical\ntrestbps - resting blood pressure (in mm Hg on admission to the hospital)\nchol - cholesterol in mg/dl\nthalach - maximum heart rate achieved\noldpeak - ST depression induced by exercise relative to rest. ‘ST’ relates to the positions on the electrocardiographic (ECG) plot.\nca - number of major vessels (0-3) colored by flouroscopy. Fluoroscopy is one of the most popular non-invasive coronary artery disease diagnosis. It enables the doctor to see the flow of blood through the coronary arteries in order to evaluate the presence of arterial blockages.\n\nCategorical attributes\n\nsex- sex\n\n1 = male\n0 = female\n\ncp- chest pain type\n\n1 = typical angina\n2 = atypical angina\n3 = non-anginal pain\n4 = asymptomatic\n\nfbs - fasting blood sugar &gt; 120 mg/dl\n\n1 = true\n0 = false\n\nrestecg - resting electrocardiographic (ECG) results\n\n0 = normal\n1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)\n2 = showing probable or definite left ventricular hypertrophy by Estes’ criteria\n\nexang - exercise induced angina. Angina is a type of chest pain caused by reduced blood flow to the heart.\n\n1 = yes\n0 = no\n\nslope - the slope of the peak exercise ST segment. (ECG)\n\n1 = upsloping\n2 = flat\n3 = downsloping\n\nthal - A blood disorder called thalassemia\n\n3 = normal blood flow\n6 = fixed defect (no blood flow in some part of the heart)\n7 = reversable defect (a blood flow is observed but it is not normal)\n\ndisease - refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. In this single dataset, ‘0’ signifies the absence of heart disease, while ‘1’ indicates diagnosed heart disease."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#exploratory-data-analysis-eda",
    "href": "posts/heart disease prediction/heart-disease.html#exploratory-data-analysis-eda",
    "title": "Heart Disease Prediction",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\nNumeric data summary\n\n\nCode\n# Summary statistics for numeric data\nnumeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n\nnumeric_summary = df[numeric_cols].describe().transpose()\nprint(\"Summary Statistics for Numeric Columns:\")\nprint(numeric_summary)\n\n# Distribution plots for numeric attributes\nplt.figure(figsize=(12, 8))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.histplot(df[col].dropna(), kde=True)\n    plt.title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n\n\nSummary Statistics for Numeric Columns:\n          count        mean         std   min    25%    50%    75%    max\nage       920.0   53.510870    9.424685  28.0   47.0   54.0   60.0   77.0\ntrestbps  861.0  132.132404   19.066070   0.0  120.0  130.0  140.0  200.0\nchol      890.0  199.130337  110.780810   0.0  175.0  223.0  268.0  603.0\nthalach   865.0  137.545665   25.926276  60.0  120.0  140.0  157.0  202.0\noldpeak   858.0    0.878788    1.091226  -2.6    0.0    0.5    1.5    6.2\nca        309.0    0.676375    0.935653   0.0    0.0    0.0    1.0    3.0\n\n\n\n\n\n\nAge: The average age in the dataset is approximately 53 years, with most individuals falling between 47 and 60 years old. The youngest person is 28 years old, and the oldest is 77 years old.\nResting Blood Pressure (trestbps): The average resting blood pressure is around 132 mm Hg, with readings typically ranging from 120 to 140 mm Hg. However, there seem to be some unusually low values (minimum at 0) that might need further investigation.\nCholesterol (chol): The average cholesterol level is about 199 mg/dl, with most values spanning between 175 and 268 mg/dl. There are also some entries with cholesterol levels recorded as 0, which might need verification.\nMaximum Heart Rate Achieved (thalach): On average, the maximum heart rate achieved is approximately 138 bpm, with the majority falling between 120 and 157 bpm.\nST Depression (oldpeak): The ST depression induced by exercise relative to rest averages around 0.88. The values range from -2.6 to 6.2.\nNumber of Major Vessels (ca): There are fewer data points available for the number of major vessels. On average, it appears that the dataset has about 0.68 major vessels colored by fluoroscopy, with values ranging from 0 to 3.\n\nGenerate a heatmap to display the correlations between different numeric attributes in heart disease dataset, helping to identify potential relationships or dependencies between these features.\n\n\nCode\n# Compute the correlation matrix\ncorr_matrix = df[numeric_cols].corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)\nplt.title('Correlation Heatmap of Numeric Features')\nplt.show()\n\n\n\n\n\n\n\nCategorical data summary\n\n\nCode\n# Categorical data summary with count plots\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'disease']\n\nplt.figure(figsize=(12, 10))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 3, i)\n    sns.countplot(data=df, x=col)\n    plt.title(f'Count Plot of {col}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCounts for each categorical attribute\n#categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'disease']\n\n#for col in categorical_cols:\n#    counts = df[col].value_counts()\n#    print(f\"Value counts for {col}:\")\n#    print(counts)\n#    print()\n\n\n\nSex: There are 726 instances of the value 1.0 (male) and 194 instances of the value 0.0 (female) in the ‘sex’ column.\nChest Pain Type (cp): 496 instances of type 4 (asymptomatic), 204 instances of type 3 (non-anginal pain), 174 instances of type 2 (atypical angina), and 46 instances of type 1 (typical angina).\nFasting Blood Sugar (fbs): 692 instances with a value of 0.0 (fasting blood sugar &lt;= 120 mg/dl) and 138 instances with a value of 1.0 (fasting blood sugar &gt; 120 mg/dl).\nResting Electrocardiographic Results (restecg): 551 instances with a value of 0.0 (normal), 188 instances with a value of 2.0 (showing probable or definite left ventricular hypertrophy by Estes’ criteria), and 179 instances with a value of 1.0 (having ST-T wave abnormality).\nExercise Induced Angina (exang): 528 instances with a value of 0.0 (no exercise-induced angina) and 337 instances with a value of 1.0 (presence of exercise-induced angina).\nSlope of Peak Exercise ST Segment (slope): 345 instances with a value of 2.0 (a flat slope), 203 instances with a value of 1.0 (an upsloping slope), and 63 instances with a value of 3.0 (a downsloping slope).\nThalassemia (thal): 196 instances with a value of 3.0 (normal blood flow), 192 instances with a value of 7.0 (reversible defect), and 46 instances with a value of 6.0 (fixed defect).\nPresence of Heart Disease (disease): 509 instances with a value of 1 (presence of heart disease) and 411 instances with a value of 0 (absence of heart disease).\n\n\n\nDifferent risk factors affect heart disease\n\n\nCode\n# Display boxplots for each numerical attribute and presence of heart disease \nplt.figure(figsize=(12, 10))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(x='disease', y=col, data=df)\n    plt.title(f'{col.capitalize()} vs Disease')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\n# Grouping by 'disease' and describing numeric attributes\nnumeric_summary = df.groupby('disease')[numeric_cols].describe().transpose()\n\n# Renaming the columns for clarity\nnumeric_summary.columns = ['non-heart disease (disease=0)', 'heart disease (disease=1)']\n\n# Displaying the summary table\nprint(numeric_summary)\n\n\n                non-heart disease (disease=0)  heart disease (disease=1)\nage      count                     411.000000                 509.000000\n         mean                       50.547445                  55.903733\n         std                         9.433700                   8.718959\n         min                        28.000000                  31.000000\n         25%                        43.000000                  51.000000\n         50%                        51.000000                  57.000000\n         75%                        57.000000                  62.000000\n         max                        76.000000                  77.000000\ntrestbps count                     391.000000                 470.000000\n         mean                      129.913043                 133.978723\n         std                        16.869867                  20.552278\n         min                        80.000000                   0.000000\n         25%                       120.000000                 120.000000\n         50%                       130.000000                 130.000000\n         75%                       140.000000                 145.000000\n         max                       190.000000                 200.000000\nchol     count                     392.000000                 498.000000\n         mean                      227.905612                 176.479920\n         std                        75.832760                 127.517611\n         min                         0.000000                   0.000000\n         25%                       199.000000                   0.000000\n         50%                       228.000000                 218.000000\n         75%                       269.000000                 267.750000\n         max                       564.000000                 603.000000\nthalach  count                     391.000000                 474.000000\n         mean                      148.800512                 128.261603\n         std                        23.608692                  24.024193\n         min                        69.000000                  60.000000\n         25%                       134.500000                 112.000000\n         50%                       151.000000                 128.000000\n         75%                       167.500000                 145.000000\n         max                       202.000000                 195.000000\noldpeak  count                     390.000000                 468.000000\n         mean                        0.418205                   1.262607\n         std                         0.715636                   1.197424\n         min                        -1.100000                  -2.600000\n         25%                         0.000000                   0.000000\n         50%                         0.000000                   1.050000\n         75%                         0.800000                   2.000000\n         max                         4.200000                   6.200000\nca       count                     165.000000                 144.000000\n         mean                        0.278788                   1.131944\n         std                         0.640006                   1.012140\n         min                         0.000000                   0.000000\n         25%                         0.000000                   0.000000\n         50%                         0.000000                   1.000000\n         75%                         0.000000                   2.000000\n         max                         3.000000                   3.000000\n\n\nFrom numerical attributes:\n\nAge: Individuals with heart disease tend to be older on average compared to those without. The median age for individuals with heart disease (57 years) is higher than for those without (51 years).\nResting Blood Pressure (trestbps): While the mean resting blood pressure appears slightly higher for individuals with heart disease, there’s overlap in the interquartile ranges, suggesting variability. However, there seem to be some unusual zero values for blood pressure in the heart disease group that might need further examination.\nCholesterol (chol): There’s a notable difference in cholesterol levels between the two groups. Individuals without heart disease have higher median cholesterol levels (228 mg/dl) compared to those with heart disease (218 mg/dl). However, there are zero values present in both groups that might need clarification.\nMaximum Heart Rate Achieved (thalach): Those without heart disease generally achieve higher maximum heart rates compared to those with heart disease. The median maximum heart rate for individuals without heart disease (151 bpm) is higher than for those with heart disease (128 bpm).\nST Depression (oldpeak): The magnitude of ST depression induced by exercise relative to rest appears significantly higher in individuals with heart disease. The median ST depression for those with heart disease (1.05) is notably greater compared to those without heart disease (0).\nNumber of Major Vessels (ca): Individuals with heart disease tend to have a higher number of major vessels colored by fluoroscopy. The median number of major vessels for those with heart disease (1) is higher than for those without heart disease (0).\nOutliers: In ‘trestbps’ and ‘chol’ columns, ‘trestbps’ displays an outlier with a minimum value of 0, while ‘chol’ has outliers represented by zero values in both categories, which may require further investigation due to their deviation from expected physiological ranges.\n\n\n\nCode\n# Generate stacked barcharts for each categorical value compared to heart disease\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n\nplt.figure(figsize=(15, 12))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 3, i)\n    ct = pd.crosstab(df[col], df['disease'], normalize='index') * 100\n    plot = ct.plot(kind='bar', stacked=True, ax=plt.gca())\n    plt.title(f'{col.capitalize()} vs Disease')\n    plt.ylabel('Percentage')\n    plt.xlabel(col.capitalize())\n    plt.legend(title='Disease', labels=['No Disease', 'Heart Disease'])\n    plt.xticks(rotation=0)\n    \n    for p in plot.patches:\n        width, height = p.get_width(), p.get_height()\n        x, y = p.get_xy() \n        plt.text(x + width / 2, \n                 y + height / 2, \n                 f'{height:.2f}%', \n                 horizontalalignment='center', \n                 verticalalignment='center')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFrom categorical attributes:\n\nSex: Females (sex=0) show a lower percentage of heart disease (25.77%) compared to males (sex=1) with a higher percentage (63.22%).\nChest Pain Type (Cp): Asymptomatic chest pain type (cp=4) displays a significantly higher percentage (79.03%) of heart disease, while typical angina (cp=1) has a lower percentage (43.48%).\nFasting Blood Sugar (Fbs): Individuals with fasting blood sugar greater than 120 mg/dl (fbs=1) tend to have a notably higher percentage (68.12%) of heart disease compared to those with lower fasting blood sugar.\nExercise Induced Angina (Exang): Individuals experiencing exercise-induced angina (exang=1) demonstrate a notably higher percentage (83.68%) of heart disease compared to those without it.\nThalassemia (Thal): Reversible defect thalassemia (thal=7) shows a higher percentage (80.21%) of heart disease compared to other types of thalassemia.\n\n\n\nSummary of EDA\nInsights from the Exploratory Data Analysis:\n\nAge Factor: Individuals diagnosed with heart disease tend to be older than those without the condition, suggesting age as a contributing risk factor for heart disease.\nChest Pain Types: Asymptomatic chest pain (cp=4) presents a notably higher prevalence of heart disease, while typical angina (cp=1) exhibits a lower association with heart conditions.\nCholesterol and Heart Disease: While cholesterol levels vary, individuals without heart disease often exhibit higher median cholesterol levels than those with the condition, although zero values in both groups require further validation.\nExercise-Induced Angina: The presence of exercise-induced angina (exang=1) demonstrates a significantly higher likelihood of heart disease compared to its absence (exang=0).\nGender Disparity: Males (sex=1) tend to show a higher percentage of heart disease cases (63.22%) compared to females (sex=0) with a lower prevalence (25.77%).\nFasting Blood Sugar (FBS): Elevated fasting blood sugar (&gt;120 mg/dl) correlates with a higher percentage of heart disease instances (68.12%) compared to lower levels.\nResting Blood Pressure and Heart Disease: Resting blood pressure, though variable, shows a trend toward higher values for heart disease cases, though outliers (0 values) necessitate further investigation."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#data-preparation",
    "href": "posts/heart disease prediction/heart-disease.html#data-preparation",
    "title": "Heart Disease Prediction",
    "section": "Data preparation",
    "text": "Data preparation\n\nFind incorrect values\n\nCholesterol (chol) contains lots of zero values.\nST Depression (oldpeak) gets some negetive values.\nResting Blood Pressure (trestbps) has one zero value.\n\nCount and plot those incorrect (missing) values.\n\n\nCode\n# Create a copy of the dataframe to handle missing values\ndf_missing = df.copy()\n\n# Replace incorrect values with NaN in specific columns\ndf_missing['chol'] = df['chol'].replace({0: np.nan})\ndf_missing['trestbps'] = df['trestbps'].replace({0: np.nan})\ndf_missing.loc[df['oldpeak'] &lt; 0, 'oldpeak'] = np.nan\n\n# Calculate missing value counts and percentages\nna_values_percent = df_missing.isna().sum().sort_values(ascending=False) \\\n    .apply(lambda x: (x, round(x / len(df_missing) * 100, 2)))\n\n# Plotting the missing value percentages\nna_values_percent.apply(lambda x: x[1]).plot.bar(title='Percentage of Missing Values in each Column')\nplt.xlabel('Columns')\nplt.ylabel('Percentage of Missing Values')\n\n# Annotating the bars with values\nfor i, val in enumerate(na_values_percent.apply(lambda x: x[1])):\n    plt.text(i, val + 1, f\"{val}%\", ha='center', va='bottom')\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Display the count and percentage of missing/incorrect values\nprint(na_values_percent)\n\n\n\n\n\nca          (611, 66.41)\nthal        (486, 52.83)\nslope       (309, 33.59)\nchol        (202, 21.96)\nfbs           (90, 9.78)\noldpeak       (74, 8.04)\ntrestbps      (60, 6.52)\nthalach       (55, 5.98)\nexang         (55, 5.98)\nrestecg        (2, 0.22)\ncp              (0, 0.0)\nsex             (0, 0.0)\nage             (0, 0.0)\ndisease         (0, 0.0)\ndtype: object\n\n\n\n\nData cleaning\nSet incorrect values to NaN and find out duplicate values to remove.\n\n\nCode\n# Create a copy of the dataframe to handle missing values and duplicates\ndf_clean = df.copy()\n\n# Replace incorrect values with NaN in specific columns\ndf_clean['chol'] = df['chol'].replace({0: np.nan})\ndf_clean['trestbps'] = df['trestbps'].replace({0: np.nan})\ndf_clean.loc[df['oldpeak'] &lt; 0, 'oldpeak'] = np.nan\n\n# Display the count of missing values before cleaning\nprint(\"Missing Values Before Cleaning:\")\nprint(df.isnull().sum())\n\n# Remove duplicate rows\nduplicate_rows = df_clean[df_clean.duplicated()]\nprint(\"\\nDuplicate Rows:\")\nprint(duplicate_rows)\n\n# Drop duplicate rows\ndf_clean.drop_duplicates(inplace=True)\n\n# Display the count of missing values after cleaning\nprint(\"\\nMissing Values After Cleaning:\")\nprint(df_clean.isnull().sum())\n\n\nMissing Values Before Cleaning:\nage           0\nsex           0\ncp            0\ntrestbps     59\nchol         30\nfbs          90\nrestecg       2\nthalach      55\nexang        55\noldpeak      62\nslope       309\nca          611\nthal        486\ndisease       0\ndtype: int64\n\nDuplicate Rows:\n      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n613  58.0  1.0  3.0     150.0  219.0  0.0      1.0    118.0    1.0      0.0   \n728  49.0  0.0  2.0     110.0    NaN  0.0      0.0    160.0    0.0      0.0   \n\n     slope  ca  thal  disease  \n613    NaN NaN   NaN        1  \n728    NaN NaN   NaN        0  \n\nMissing Values After Cleaning:\nage           0\nsex           0\ncp            0\ntrestbps     60\nchol        201\nfbs          90\nrestecg       2\nthalach      55\nexang        55\noldpeak      74\nslope       307\nca          609\nthal        484\ndisease       0\ndtype: int64"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#feature-engineering",
    "href": "posts/heart disease prediction/heart-disease.html#feature-engineering",
    "title": "Heart Disease Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nData imputation\n\n\nCode\n# Columns for mode imputation\nmode_cols = ['restecg', 'exang', 'fbs', 'slope', 'thal']\n\n# Columns for median imputation\nmedian_cols = ['oldpeak', 'trestbps', 'thalach', 'chol', 'ca']\n\n# Impute missing values with mode for mode_cols\nmode_imputer = SimpleImputer(strategy='most_frequent')\ndf_clean[mode_cols] = mode_imputer.fit_transform(df_clean[mode_cols])\n\n# Impute missing values with median for median_cols\nmedian_imputer = SimpleImputer(strategy='median')\ndf_clean[median_cols] = median_imputer.fit_transform(df_clean[median_cols])\n\n# Verify the imputed values\nprint(\"Missing Values After Imputation:\")\nprint(df_clean.isnull().sum())\n\n\nMissing Values After Imputation:\nage         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ndisease     0\ndtype: int64\n\n\n\n\nCode\n# Check the number of rows after data imputation\nnum_rows_after_imputation = df_clean.shape[0]\nprint(f\"Number of rows after data imputation: {num_rows_after_imputation}\")\n\n\nNumber of rows after data imputation: 918\n\n\nThe total number of rows before the data split was 920. Since only two duplicate rows were removed, the remaining rows became 918.\n\n\nOne-hot encoding\nIt’s important to apply One-Hot Encoding before splitting data into training and testing sets. Doing this ensures that each set is independently encoded, preventing any mixing of information between them. This separation is important because it helps the model learn and make predictions correctly without being influenced by how the data was divided.\n\n\nCode\n# Categorical columns for one-hot encoding\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n\n# Apply one-hot encoding\ndf_encoded = pd.get_dummies(df_clean, columns=categorical_cols)\n\n# Display the first few rows to verify the encoding\nprint(df_encoded.head())\n\n\n    age  trestbps   chol  thalach  oldpeak   ca  disease  sex_0.0  sex_1.0  \\\n0  32.0      95.0  240.0    127.0      0.7  0.0        1    False     True   \n1  34.0     115.0  240.0    154.0      0.2  0.0        1    False     True   \n2  35.0     130.0  240.0    130.0      0.5  0.0        1    False     True   \n3  36.0     110.0  240.0    125.0      1.0  0.0        1    False     True   \n4  38.0     105.0  240.0    166.0      2.8  0.0        1     True    False   \n\n   cp_1.0  ...  restecg_1.0  restecg_2.0  exang_0.0  exang_1.0  slope_1.0  \\\n0    True  ...        False        False       True      False       True   \n1   False  ...        False        False       True      False       True   \n2   False  ...        False        False      False       True      False   \n3   False  ...        False        False      False       True      False   \n4   False  ...        False        False       True      False       True   \n\n   slope_2.0  slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n0      False      False      True     False     False  \n1      False      False      True     False     False  \n2       True      False     False     False      True  \n3       True      False     False      True     False  \n4      False      False      True     False     False  \n\n[5 rows x 26 columns]\n\n\n\n\nFeature scaling & Split dataset\nNormalization / Standardization While ML algorithms like Random Forest, linear regression, logistic regression, and neural networks don’t require scaling, it’s crucial for Distance algorithms such as KNN, K-means, and SVM.\nTo accommodate this, I’ll split the dataset: one portion for KNN and logistic regression, where normalization is employed, and another for RF modeling, where scaling isn’t necessary.\n70% for Train & 30% for Test\n\n\nCode\n# Select numerical attributes for scaling\nnumerical_cols = ['age', 'oldpeak', 'chol', 'thalach', 'trestbps']\n\n# Extract features and target variable\nX = df_encoded.drop('disease', axis=1)\ny = df_encoded['disease']\n\n# Splitting the dataset into train and test sets (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Separate the numerical columns for scaling\nX_train_numeric = X_train[numerical_cols]\nX_test_numeric = X_test[numerical_cols]\n\n# Feature scaling for numerical attributes for KNN and Logistic Regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_numeric)\nX_test_scaled = scaler.transform(X_test_numeric)\n\n# Replace the scaled numerical columns in the train and test sets for KNN and Logistic Regression\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_cols, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_cols, index=X_test.index)\n\nX_train_knn_logreg = X_train.copy()\nX_test_knn_logreg = X_test.copy()\n\nX_train_knn_logreg[numerical_cols] = X_train_scaled\nX_test_knn_logreg[numerical_cols] = X_test_scaled"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#modeling",
    "href": "posts/heart disease prediction/heart-disease.html#modeling",
    "title": "Heart Disease Prediction",
    "section": "Modeling",
    "text": "Modeling\n\nK-Nearest Neighbors (KNN)\n\n\nCode\nknn = KNeighborsClassifier()\nknn.fit(X_train_knn_logreg, y_train)\ny_pred_knn = knn.predict(X_test_knn_logreg)\n\n\n/Users/harrywang/.virtualenvs/r-reticulate/lib/python3.9/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.9 is maybe too old for this OS.\n  warnings.warn(\n\n\n\n\nLogistic Regression\n\n\nCode\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_knn_logreg, y_train)\ny_pred_log_reg = log_reg.predict(X_test_knn_logreg)\n\n\n\n\nRandom Forest (data used without scaling)\n\n\nCode\nrf = RandomForestClassifier(random_state=37)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#evaluation-models-performance",
    "href": "posts/heart disease prediction/heart-disease.html#evaluation-models-performance",
    "title": "Heart Disease Prediction",
    "section": "Evaluation models performance",
    "text": "Evaluation models performance\n\n\nCode\nmodels = {\n    \"KNN\": (y_test, y_pred_knn),\n    \"Logistic Regression\": (y_test, y_pred_log_reg),\n    \"Random Forest\": (y_test, y_pred_rf)\n}\n\nfor model_name, (true, pred) in models.items():\n    accuracy = accuracy_score(true, pred)\n    precision = precision_score(true, pred)\n    recall = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    auc = roc_auc_score(true, pred)\n    \n    print(f\"Metrics for {model_name}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"AUC Score: {auc:.4f}\")\n    print(\"\\n\")\n\n\nMetrics for KNN:\nAccuracy: 0.8043\nPrecision: 0.7829\nRecall: 0.8954\nF1 Score: 0.8354\nAUC Score: 0.7932\n\n\nMetrics for Logistic Regression:\nAccuracy: 0.7935\nPrecision: 0.7824\nRecall: 0.8693\nF1 Score: 0.8235\nAUC Score: 0.7842\n\n\nMetrics for Random Forest:\nAccuracy: 0.8152\nPrecision: 0.8072\nRecall: 0.8758\nF1 Score: 0.8401\nAUC Score: 0.8078\n\n\n\n\n\n\nCode\n# Define a function to plot confusion matrix\ndef plot_confusion(y_true, y_pred, title):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n# Plot confusion matrix for KNN\nplot_confusion(y_test, y_pred_knn, 'Confusion Matrix - KNN')\n\n# Plot confusion matrix for Logistic Regression\nplot_confusion(y_test, y_pred_log_reg, 'Confusion Matrix - Logistic Regression')\n\n# Plot confusion matrix for Random Forest\nplot_confusion(y_test, y_pred_rf, 'Confusion Matrix - Random Forest')\n\n\n\n\n\n\n\n\n\n\n\nThe table display the model evaluation comparison.\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nRecall\nPrecision\nF1 Score\nAUC Score\n\n\n\n\nKNN\n0.8043\n0.8954\n0.7829\n0.8354\n0.7932\n\n\nLogistic Regression\n0.7935\n0.8693\n0.7824\n0.8235\n0.7842\n\n\nRandom Forest\n0.8152\n0.8758\n0.8072\n0.8401\n0.8078\n\n\n\n\nAccuracy Comparison: Random Forest achieved the highest accuracy (81.52%), outperforming both KNN (80.43%) and Logistic Regression (79.35%).\nPrecision and Recall: Random Forest and KNN showed similar recall rates of 87.58%, signifying their proficiency in capturing positive cases. However, Random Forest showcased superior precision at 80.72% compared to KNN’s 78.29% and Logistic Regression’s 78.24%. This highlights Random Forest’s better balance between precision and recall in correctly identifying positive cases while minimizing false positives.\nF1 Score: Random Forest yielded the highest F1 score (84.01%), signifying a balanced performance between precision and recall, followed by KNN (83.54%) and Logistic Regression (82.35%).\nAUC Score: Random Forest had the highest AUC score (80.78%), indicating its capability to distinguish between classes better than KNN (79.32%) and Logistic Regression (78.42%).\nRandom Forest Model: Overall, Random Forest demonstrated superior performance across multiple metrics, showcasing its robustness in predicting heart disease compared to the other models."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#conclusion",
    "href": "posts/heart disease prediction/heart-disease.html#conclusion",
    "title": "Heart Disease Prediction",
    "section": "Conclusion",
    "text": "Conclusion\n\nEmploying KNN, Logistic Regression, and Random Forest algorithms, models were constructed to predict heart disease with accuracies ranging between 79.35% and 81.52%. Notably, the Random Forest model emerged as the most effective model, exhibiting an accuracy of 81.52% and an F1 score of 84.01%, showcasing its robustness in identifying heart disease cases.\nThrough comprehensive exploratory data analysis (EDA), important insights were revealed. Factors such as age, chest pain type, cholesterol levels, and exercise-induced angina showed notable correlations with the presence of heart disease. Visualizing these relationships aided in understanding risk factors associated with heart conditions.\nThe data preparation phase involved handling missing values, erroneous entries, and duplicates. Feature engineering encompassed imputation of missing values, one-hot encoding for categorical variables, and scaling for suitable algorithms, ensuring the data was ready for modeling.\nThe significance of various attributes in predicting heart disease was evident. Numeric attributes like age, cholesterol levels, and ST depression were influential, while categorical features such as chest pain type and thalassemia also played crucial roles. Random Forest, exhibiting superior performance, emphasized the importance of these features in predicting heart disease.\nThe project’s success in predictive modeling offers potential clinical implications. These models can aid healthcare practitioners by providing insights into patient risk factors for heart disease. However, further refinement through hyperparameter tuning and larger datasets could enhance predictive accuracy, paving the way for more reliable clinical decision support systems."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Heart Disease Prediction\n\n\n\npython\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\nHuang Han Wang\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/research project story/research-story.html",
    "href": "blog/research project story/research-story.html",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "",
    "text": "Figure 1: Rainforest Discovery Center (RDC) @Sepilok, Malaysia"
  },
  {
    "objectID": "blog/research project story/research-story.html#introduction",
    "href": "blog/research project story/research-story.html#introduction",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "Introduction",
    "text": "Introduction\nThe story all starts from here, in east Malaysia Borneo island. In Chinese, there is an old quote saying that “天地有大美而不言” by the Chinese philosopher Zhuangzi, which means the universe and the mother earth have their natural beauty, but they won’t say any word to others. There is such a word in English: Extraordinary meaning beyond normal and surpassing ordinary. Also, in Bahasa Melayu, “Luarbiasa” contains the same meaning: luar means further away, and biasa means ordinary.\nThe first time I visited the tropical rainforest, I was amazed by the HUGE things there. For instance, anything is big here: trees, leaves, flowers, fruits, and creatures (like ants, orangutans, birds, and so on). My 5-day rainforest journey, organized by Society of Wilderness Malaysia, began from the 1st to 5th March 2023, and this also motivates me to engage more in nature conservation efforts."
  },
  {
    "objectID": "blog/research project story/research-story.html#insights",
    "href": "blog/research project story/research-story.html#insights",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "Insights",
    "text": "Insights\nThere are many beautiful parts of the Malaysian rainforest. In this article, I am going to talk about three parts: “Tree Canopy,” “Creatures,” and “My Observation.”\nMalaysia is well known for its rich biodiversity. Let’s be ready and dive into it together!\n\nTree Canopy\nWhen it comes to rainforests, the lungs of the earth and the “broccoli” tree pop up in my mind.\n\n\n\nFigure 2: Canopy Walkway in RDC @Sepilok, Malaysia\n\n\nUsually, the tree structure of a rainforest can be divided into emergents, canopy, under canopy, and shrub layers. Thus, most reserve areas set up a canopy walkway to make it easier for people to observe nature; as Figure 2 displays, the trunks of the trees are typically thin in the rainforest. Let me give a quick overview of the Rainforest Discovery Center (RDC) in Sepilok. Situated on the edge of the Kabili-Sepilok Forest Reserve in Sandakan, RDC has been operating since 1996, focusing on environmental education. Today, it’s a 3-in-1 park catering to wildlife, bird, and plant enthusiasts, offering a glimpse of Borneo’s distinctive biodiversity for nature lovers and bird watchers.\n\n\n\n\n\n\n\n(a) Height of Rainforest\n\n\n\n\n\n\n\n(b) Layers of Rainforest\n\n\n\n\nFigure 3: Layers of Rainforest with meters. sources from internet geography & sciencefacts.net\n\n\nThere are some interesting points to note from Figure 3. Tropical canopies, reaching heights of up to over 50 meters (about 16-17 floors high), showcase their competitive nature in absorbing sunlight for rapid growth. This is reflected in their thin trunks, making them become giant umbrella-like forms. The canopy layer, constituting the second rainforest level, serves as a shield, intercepting light penetrating from the emergent layer above. Approximately 75-98% of light is absorbed here, effectively blocking it from reaching the lower layers.\nMoreover, the canopy layer acts as a natural reservoir, capturing most rainfall, which limits the amount that reaches the plants in the lower strata. Therefore, life predominantly thrives in this layer, while vegetation below relies on the scant rainfall that trickles down from above.\nFor more study, please refer to Rainforest - A rainforest is an area of tall trees and a high amount of rainfall by National Geographic.\n\n\nUnique Creatures\nBiodiversity is undoubtedly amazing in Malaysia, which is also the most exciting part of this nature observation journey. In the following, I will show some pictures of signature creatures. As I was using binoculars as my second eye to enjoy the beauty of nature, it would be more engaging to look through from my point of view and describe how amazing it is in this beautiful heaven.\n\n\n\nFigure 4: Sitting on a small boat on Kinabatangan River\n\n\nThe Kinabatangan River (Malay: Sungai Kinabatangan) stands as Malaysian second-longest river, stretching across 560 kilometers from its origins in the southwest Sabah mountains to its convergence with the Sulu Sea, east of Sandakan. This region is renowned for its remarkable biodiversity, boasting a rich tapestry of natural wonders, including limestone caves nestled in Gomantong Hill, expansive dryland dipterocarp forests, lush riverine and freshwater swamp forests, picturesque oxbow lakes, and the brackish mangrove swamps hugging the coastal areas.\n\n\n\n\n\n\nNote\n\n\n\nNote that images were taken through binoculars. Please bear with the quality. 😃\n\n\n\n\n\n\n\n\n\n(a) Orangutan eating figs\n\n\n\n\n\n\n\n(b) Proboscis Monkey sitting on a tree near the river\n\n\n\n\nFigure 5: Wild orangutan and proboscis monkey\n\n\nWe spent two days on the Kinabatangan River to observe the unique creatures in Malaysia. The first two superstars, the orangutan and proboscis monkey, are well known worldwide and stand as symbols for Malaysia. I will showcase the pictures of these two creatures in the conservation area to compare each other.\n\n\n\n\n\n\n\n(a) The couple of Oriental pied hornbill\n\n\n\n\n\n\n\n(b) Borneo elephant bathing near the river\n\n\n\n\nFigure 6: Wild hornbill and Borneo elephant\n\n\nHornbills are known as Malaysia National Birds. There are 10 species of hornbills found in Malaysia, and they typically come together as a couple. The Borneo elephant is found on the island of Borneo in Malaysia. They are a type of Asian elephant, but smaller in size, about half the size of an Asian elephant. Borneo elephants inhabit tropical rainforests and swampy areas, feeding on leaves, grass, and fruits.\n\n\n\n\n\n\n\n(a) Malayan flying lemur at RDC @Sepilok\n\n\n\n\n\n\n\n(b) Tarsier during night time at RDC @Sepilok\n\n\n\n\nFigure 7: Sunda flying lemur and Tarsier are both arboreal creatures, living in trees, and being nocturnal in their activities.\n\n\n\n\n\n\n\n\n\n(a) Malayan Sun bear at Bornean Sun Bear Conservation Centre\n\n\n\n\n\n\n\n(b) Kingfisher at Sepilok Jungle Resort\n\n\n\n\nFigure 8: Sun bear and beautiful Kingfisher\n\n\nSun bears, also called the ‘Malayan Sun bear,’ are the smallest member of the bear family with a long tongue and experience in climbing trees. They lumber through the forests by night, snacking on fruits, berries, roots, insects, small birds, lizards, and rodents.\nFrom what I heard from Calvin Soh, the President of the Society of Wilderness Malaysia, we can find more than 20 species of kingfishers across Malaysia if we keep putting effort into observing nature.\n\n\n\n\n\n\n\n(a) Proboscis Monkey at labuk Bay Proboscis Monkey Sanctuary @Sandakan\n\n\n\n\n\n\n\n(b) Orangutan at Sepilok Orangutan Rehabilitation Centre @Sepilok\n\n\n\n\nFigure 9: Both images were taken during food time, and orangutans and proboscis monkeys were conserved in the reserved area.\n\n\nBy contrast, it is more exciting for me to observe the animals in the wild since we have to be focused and enjoy every moment when doing a safari. I noticed that wild animals, such as orangutans and proboscis monkeys, are more active and lively than those in the conversation area. Most importantly, wild animals grow bigger than animals in conserved regions.\nTo sum up, rich biodiversity is the only word that pops up in my mind, and many pictures are still stored on my device and memorized in my mind. Thus, rainforests and canopies play an essential role in their habitat. One of the purposes of writing this article is to raise awareness among the public and be informed of the beauty of nature.\n\n\nMy Observation\n\nDeforestation in Borneo Island\nAfter those lovely animals, let’s discuss some serious issues as I try to engage more in this topic. Unavoidably, when it comes to global warming, trees and forests often come to the discussion table.\nThere are some significant data to show.\n\n\n\nFigure 10: Tree distribution area map in Borneo from 1950 to 2020 (prediction).source from Hugo Ahlenius online\n\n\n\n\n\nFigure 11: Deforestation in Borneo (hectares). source from Rhett A. Butler online\n\n\nIn Malaysia, the primary forest and tree cover data showcases distinct perspectives on forest landscapes. The primary forest cover in 2001 stood at 10,611,815 hectares, whereas by 2020, it had declined to 8,708,737 hectares, indicating a significant loss of 1,903,078 hectares or approximately 17.9%. This reduction is a cause for concern, representing a considerable portion of Malaysia’s original forested areas.\nConversely, when assessing the broader tree cover across Malaysia, the figures reflect a similar trend but encompass a larger expanse. In 2001, the total tree cover was noted at 18,164,118 hectares, which then diminished to 15,243,971 hectares by 2020, revealing a loss of 4,403,860 hectares or around 24.2% over the period between 2002 and 2019.\nThe primary forest remains undisturbed, vital for biodiversity and ecosystem stability, while tree cover includes disturbed areas like logged or altered land. Deforestation due to agriculture, logging, and urbanization primarily affects primary forests. Yet, changes in tree cover may involve reforestation or secondary growth, offsetting some losses. Distinguishing between them is crucial: primary forest loss poses an immediate threat to ecosystems, needing targeted conservation. Meanwhile, broader tree cover changes encompass various land use shifts, demanding comprehensive forest management strategies. Both are vital for understanding forest dynamics and their impact on biodiversity and the environment.\n\n\nTrees play an essential role in carbon storage\nNow, after discussing the deforestation issue around Borneo Island, let’s dive into how important it is for trees to absorb carbon. I must say trees and the ocean are critically crucial for carbon stock regarding global warming. We will prove it in Figure 12 below.\n\n\n\n\n\n\n\n(a) Carbon Storage Circle\n\n\n\n\n\n\n\n(b) Carbon Budget (2007-2016)\n\n\n\n\nFigure 12: Carbon storage (circle) and carbon budget. sources from Scottish Centre for Carbon Storage & Global Carbon Budget\n\n\nFrom both images, atmospheric carbon dioxide (CO2) is absorbed mainly by land sinks, which are trees, grass, and earth, and then the ocean sink plays a secondary role in digesting CO2.\nAs we notice, plants and the ocean can use photosynthesis to recycle CO2. After photosynthesis, carbon dioxide is stored in the earth as fossil carbon in geological reservoirs.\nHowever, during the ten years (2007-2016), human release of carbon dioxide was far more than land and ocean stores. Fossil fuels and industry contain most of it, three times more than trees store for carbon stock.\nOverall, the importance of photosynthesis from trees allows the planet to recycle carbon dioxide and release oxygen back. In the tropical rainforest regions, the logging industry and industry development are the main issues to be solved.\nAfter understanding the carbon recycle flow figure, let’s explain more about the components of the tree. This is also related to above-ground biomass density (AGBD), which refers to the weight of living things like trees and plants above the soil. It’s a measure of how much greenery exists in a specific area. AGB helps us understand the amount of carbon stored in forests, which is crucial for fighting climate change. Measuring AGB is like weighing the ‘green power’ of trees, showing how much they help keep our planet healthy.\n\n\n\n\n\n\n\n(a) The chemical composition of wood\n\n\n\n\n\n\n\n(b) Parts of a Tree\n\n\n\n\nFigure 13: Component of tree. sources from Matt Russell & Georgette Kilgore\n\n\nAs the pictures show, it’s obvious that half of the tree’s biomass is carbon, followed by oxygen, at around 44%. This is also said by Ralph Dubayah (NASA GEDI Mission Principal Investigator). The green power of trees contains carbon not only in the wood but also in their leaves, twigs, branches, roots, and so on.\nUnderstanding the tree’s above-ground biomass density (AGBD) becomes essential—it measures the greenery’s weight, indicative of carbon storage critical for addressing climate change. Trees’ green power, containing roughly half of their biomass as carbon, underscores their immense contribution to our planet’s health and fight against climate change.\n\n\nSatellite Imageries can help with this matter\nNowadays, as technology and computation power grow, it is possible to rely on big data processing to deal with in-time abundant satellite image data with high-resolution quality.\n\n\n\n\n\n\n\n(a) Satellite above the Earth\n\n\n\n\n\n\n\n(b) Simulation of GEDI lasers collecting data\n\n\n\n\nFigure 14: The power of satellite images data. sources from UMD\n\n\nNASA’s GEDI (Global Ecosystem Dynamics Investigation) mission is a groundbreaking initiative employing advanced laser technology aboard the International Space Station. GEDI precisely measures the height of Earth’s forests, providing accurate data on L2A canopy top height. This technology revolutionizes forest monitoring by offering detailed information on forest structure and biomass. Traditional methods for measuring canopy height and above-ground biomass density (AGBD) involved ground-based measurements or aerial surveys, which were time-consuming and less comprehensive.\nGEDI’s innovative approach allows for global coverage, overcoming the limitations of ground-based techniques. By using lasers to measure canopy height, GEDI generates highly accurate 3D maps of forests, offering insights into their structure and carbon storage potential. This satellite-based method significantly enhances our understanding of forests’ carbon storage capacity and their role in mitigating climate change.\nComparing traditional methods to GEDI’s satellite approach, the latter provides a broader, more consistent, and cost-effective way to assess forests globally. It offers continuous monitoring, whereas ground-based methods often cover limited areas and can be labor-intensive. GEDI’s data allows scientists to track changes in canopy height and AGBD over time, enabling better forest management strategies crucial for preserving our planet’s health.\nThis is also an intriguing way for me to figure out a new way to measure carbon stock instead of human power needed and time-consuming. Exploring this innovative method to measure carbon stock is not just about replacing labor-intensive processes; it represents a leap forward in efficiency and accuracy. With GEDI’s precise data on canopy height and biomass, assessing carbon storage becomes streamlined, allowing for more comprehensive and timely monitoring. This shift from traditional methods to satellite-based technology is not only forward-thinking but also empowers us to better protect and sustain our planet’s invaluable forests."
  },
  {
    "objectID": "blog/research project story/research-story.html#summary",
    "href": "blog/research project story/research-story.html#summary",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "Summary",
    "text": "Summary\n\n\n\nFigure 15: Observing nature through my binoculars.\n\n\nReflecting on my journey in Malaysia, it’s the blend of cultures and rich biodiversity that makes this country unique. With influences from East and West—Chinese, Islamic, Indian, and Western—the mix of traditions and natural beauty creates a diverse and fascinating landscape.\nExploring this diverse and nature-rich country has been an eye-opening experience. Nature acts as our best teacher, guiding and teaching us invaluable lessons through its vast biodiversity—towering rainforests and captivating wildlife.\nThe topic, “Fascinating Malaysia Biodiversity,” encourages us to eagerly anticipate and cherish each moment. It’s a reminder to value and protect the delicate balance of life on our planet, nurturing a harmonious relationship between humans and nature. In this connection, we find not just fascination but a deep sense of appreciation and responsibility toward our incredible natural world."
  },
  {
    "objectID": "culture.html",
    "href": "culture.html",
    "title": "Cultural Oasis",
    "section": "",
    "text": "No matching items"
  }
]