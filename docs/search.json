[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Fascinating Malaysia Biodiversity - Research Motivation\n\n\n\n\n\n\n\nmalaysia\n\n\nbiodiversity\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\nHuang Han Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "culture.html",
    "href": "culture.html",
    "title": "Cultural Oasis",
    "section": "",
    "text": "Immersing Indian Culture - Garden of Gods - Penang (George Town)\n\n\n\nIndian culture\n\n\nforum\n\n\nMalaysia\n\n\n\n\n\n\n\nHuang Han Wang\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/research project story/research-story.html",
    "href": "blog/research project story/research-story.html",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "",
    "text": "Figure¬†1: Rainforest Discovery Center (RDC) @Sepilok, Malaysia"
  },
  {
    "objectID": "blog/research project story/research-story.html#introduction",
    "href": "blog/research project story/research-story.html#introduction",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "Introduction",
    "text": "Introduction\nThe story all starts from here, in east Malaysia Borneo island. In Chinese, there is an old quote saying that ‚ÄúÂ§©Âú∞ÊúâÂ§ßÁæéËÄå‰∏çË®Ä‚Äù by the Chinese philosopher Zhuangzi, which means the universe and the mother earth have their natural beauty, but they won‚Äôt say any word to others. There is such a word in English: Extraordinary meaning beyond normal and surpassing ordinary. Also, in Bahasa Melayu, ‚ÄúLuarbiasa‚Äù contains the same meaning: luar means further away, and biasa means ordinary.\nThe first time I visited the tropical rainforest, I was amazed by the HUGE things there. For instance, anything is big here: trees, leaves, flowers, fruits, and creatures (like ants, orangutans, birds, and so on). My 5-day rainforest journey, organized by Society of Wilderness Malaysia, began from the 1st to 5th March 2023, and this also motivates me to engage more in nature conservation efforts."
  },
  {
    "objectID": "blog/research project story/research-story.html#insights",
    "href": "blog/research project story/research-story.html#insights",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "Insights",
    "text": "Insights\nThere are many beautiful parts of the Malaysian rainforest. In this article, I am going to talk about three parts: ‚ÄúTree Canopy,‚Äù ‚ÄúCreatures,‚Äù and ‚ÄúMy Observation.‚Äù\nMalaysia is well known for its rich biodiversity. Let‚Äôs be ready and dive into it together!\n\nTree Canopy\nWhen it comes to rainforests, the lungs of the earth and the ‚Äúbroccoli‚Äù tree pop up in my mind.\n\n\n\nFigure¬†2: Canopy Walkway in RDC @Sepilok, Malaysia\n\n\nUsually, the tree structure of a rainforest can be divided into emergents, canopy, under canopy, and shrub layers. Thus, most reserve areas set up a canopy walkway to make it easier for people to observe nature; as Figure 2 displays, the trunks of the trees are typically thin in the rainforest. Let me give a quick overview of the Rainforest Discovery Center (RDC) in Sepilok. Situated on the edge of the Kabili-Sepilok Forest Reserve in Sandakan, RDC has been operating since 1996, focusing on environmental education. Today, it‚Äôs a 3-in-1 park catering to wildlife, bird, and plant enthusiasts, offering a glimpse of Borneo‚Äôs distinctive biodiversity for nature lovers and bird watchers.\n\n\n\n\n\n\n\n(a) Height of Rainforest\n\n\n\n\n\n\n\n(b) Layers of Rainforest\n\n\n\n\nFigure¬†3: Layers of Rainforest with meters. sources from internet geography & sciencefacts.net\n\n\nThere are some interesting points to note from Figure 3. Tropical canopies, reaching heights of up to over 50 meters (about 16-17 floors high), showcase their competitive nature in absorbing sunlight for rapid growth. This is reflected in their thin trunks, making them become giant umbrella-like forms. The canopy layer, constituting the second rainforest level, serves as a shield, intercepting light penetrating from the emergent layer above. Approximately 75-98% of light is absorbed here, effectively blocking it from reaching the lower layers.\nMoreover, the canopy layer acts as a natural reservoir, capturing most rainfall, which limits the amount that reaches the plants in the lower strata. Therefore, life predominantly thrives in this layer, while vegetation below relies on the scant rainfall that trickles down from above.\nFor more study, please refer to Rainforest - A rainforest is an area of tall trees and a high amount of rainfall by National Geographic.\n\n\nUnique Creatures\nBiodiversity is undoubtedly amazing in Malaysia, which is also the most exciting part of this nature observation journey. In the following, I will show some pictures of signature creatures. As I was using binoculars as my second eye to enjoy the beauty of nature, it would be more engaging to look through from my point of view and describe how amazing it is in this beautiful heaven.\n\n\n\nFigure¬†4: Sitting on a small boat on Kinabatangan River\n\n\nThe Kinabatangan River (Malay: Sungai Kinabatangan) stands as Malaysian second-longest river, stretching across 560 kilometers from its origins in the southwest Sabah mountains to its convergence with the Sulu Sea, east of Sandakan. This region is renowned for its remarkable biodiversity, boasting a rich tapestry of natural wonders, including limestone caves nestled in Gomantong Hill, expansive dryland dipterocarp forests, lush riverine and freshwater swamp forests, picturesque oxbow lakes, and the brackish mangrove swamps hugging the coastal areas.\n\n\n\n\n\n\nNote\n\n\n\nNote that images were taken through binoculars. Please bear with the quality. üòÉ\n\n\n\n\n\n\n\n\n\n(a) Orangutan eating figs\n\n\n\n\n\n\n\n(b) Proboscis Monkey sitting on a tree near the river\n\n\n\n\nFigure¬†5: Wild orangutan and proboscis monkey\n\n\nWe spent two days on the Kinabatangan River to observe the unique creatures in Malaysia. The first two superstars, the orangutan and proboscis monkey, are well known worldwide and stand as symbols for Malaysia. I will showcase the pictures of these two creatures in the conservation area to compare each other.\n\n\n\n\n\n\n\n(a) The couple of Oriental pied hornbill\n\n\n\n\n\n\n\n(b) Borneo elephant bathing near the river\n\n\n\n\nFigure¬†6: Wild hornbill and Borneo elephant\n\n\nHornbills are known as Malaysia National Birds. There are 10 species of hornbills found in Malaysia, and they typically come together as a couple. The Borneo elephant is found on the island of Borneo in Malaysia. They are a type of Asian elephant, but smaller in size, about half the size of an Asian elephant. Borneo elephants inhabit tropical rainforests and swampy areas, feeding on leaves, grass, and fruits.\n\n\n\n\n\n\n\n(a) Malayan flying lemur at RDC @Sepilok\n\n\n\n\n\n\n\n(b) Tarsier during night time at RDC @Sepilok\n\n\n\n\nFigure¬†7: Sunda flying lemur and Tarsier are both arboreal creatures, living in trees, and being nocturnal in their activities.\n\n\n\n\n\n\n\n\n\n(a) Malayan Sun bear at Bornean Sun Bear Conservation Centre\n\n\n\n\n\n\n\n(b) Kingfisher at Sepilok Jungle Resort\n\n\n\n\nFigure¬†8: Sun bear and beautiful Kingfisher\n\n\nSun bears, also called the ‚ÄòMalayan Sun bear,‚Äô are the smallest member of the bear family with a long tongue and experience in climbing trees. They lumber through the forests by night, snacking on fruits, berries, roots, insects, small birds, lizards, and rodents.\nFrom what I heard from Calvin Soh, the President of the Society of Wilderness Malaysia, we can find more than 20 species of kingfishers across Malaysia if we keep putting effort into observing nature.\n\n\n\n\n\n\n\n(a) Proboscis Monkey at labuk Bay Proboscis Monkey Sanctuary @Sandakan\n\n\n\n\n\n\n\n(b) Orangutan at Sepilok Orangutan Rehabilitation Centre @Sepilok\n\n\n\n\nFigure¬†9: Both images were taken during food time, and orangutans and proboscis monkeys were conserved in the reserved area.\n\n\nBy contrast, it is more exciting for me to observe the animals in the wild since we have to be focused and enjoy every moment when doing a safari. I noticed that wild animals, such as orangutans and proboscis monkeys, are more active and lively than those in the conversation area. Most importantly, wild animals grow bigger than animals in conserved regions.\nTo sum up, rich biodiversity is the only word that pops up in my mind, and many pictures are still stored on my device and memorized in my mind. Thus, rainforests and canopies play an essential role in their habitat. One of the purposes of writing this article is to raise awareness among the public and be informed of the beauty of nature.\n\n\nMy Observation\n\nDeforestation in Borneo Island\nAfter those lovely animals, let‚Äôs discuss some serious issues as I try to engage more in this topic. Unavoidably, when it comes to global warming, trees and forests often come to the discussion table.\nThere are some significant data to show.\n\n\n\nFigure¬†10: Tree distribution area map in Borneo from 1950 to 2020 (prediction).source from Hugo Ahlenius online\n\n\n\n\n\nFigure¬†11: Deforestation in Borneo (hectares). source from Rhett A. Butler online\n\n\nIn Malaysia, the primary forest and tree cover data showcases distinct perspectives on forest landscapes. The primary forest cover in 2001 stood at 10,611,815 hectares, whereas by 2020, it had declined to 8,708,737 hectares, indicating a significant loss of 1,903,078 hectares or approximately 17.9%. This reduction is a cause for concern, representing a considerable portion of Malaysia‚Äôs original forested areas.\nConversely, when assessing the broader tree cover across Malaysia, the figures reflect a similar trend but encompass a larger expanse. In 2001, the total tree cover was noted at 18,164,118 hectares, which then diminished to 15,243,971 hectares by 2020, revealing a loss of 4,403,860 hectares or around 24.2% over the period between 2002 and 2019.\nThe primary forest remains undisturbed, vital for biodiversity and ecosystem stability, while tree cover includes disturbed areas like logged or altered land. Deforestation due to agriculture, logging, and urbanization primarily affects primary forests. Yet, changes in tree cover may involve reforestation or secondary growth, offsetting some losses. Distinguishing between them is crucial: primary forest loss poses an immediate threat to ecosystems, needing targeted conservation. Meanwhile, broader tree cover changes encompass various land use shifts, demanding comprehensive forest management strategies. Both are vital for understanding forest dynamics and their impact on biodiversity and the environment.\n\n\nTrees play an essential role in carbon storage\nNow, after discussing the deforestation issue around Borneo Island, let‚Äôs dive into how important it is for trees to absorb carbon. I must say trees and the ocean are critically crucial for carbon stock regarding global warming. We will prove it in Figure 12 below.\n\n\n\n\n\n\n\n(a) Carbon Storage Circle\n\n\n\n\n\n\n\n(b) Carbon Budget (2007-2016)\n\n\n\n\nFigure¬†12: Carbon storage (circle) and carbon budget. sources from Scottish Centre for Carbon Storage & Global Carbon Budget\n\n\nFrom both images, atmospheric carbon dioxide (CO2) is absorbed mainly by land sinks, which are trees, grass, and earth, and then the ocean sink plays a secondary role in digesting CO2.\nAs we notice, plants and the ocean can use photosynthesis to recycle CO2. After photosynthesis, carbon dioxide is stored in the earth as fossil carbon in geological reservoirs.\nHowever, during the ten years (2007-2016), human release of carbon dioxide was far more than land and ocean stores. Fossil fuels and industry contain most of it, three times more than trees store for carbon stock.\nOverall, the importance of photosynthesis from trees allows the planet to recycle carbon dioxide and release oxygen back. In the tropical rainforest regions, the logging industry and industry development are the main issues to be solved.\nAfter understanding the carbon recycle flow figure, let‚Äôs explain more about the components of the tree. This is also related to above-ground biomass density (AGBD), which refers to the weight of living things like trees and plants above the soil. It‚Äôs a measure of how much greenery exists in a specific area. AGB helps us understand the amount of carbon stored in forests, which is crucial for fighting climate change. Measuring AGB is like weighing the ‚Äògreen power‚Äô of trees, showing how much they help keep our planet healthy.\n\n\n\n\n\n\n\n(a) The chemical composition of wood\n\n\n\n\n\n\n\n(b) Parts of a Tree\n\n\n\n\nFigure¬†13: Component of tree. sources from Matt Russell & Georgette Kilgore\n\n\nAs the pictures show, it‚Äôs obvious that half of the tree‚Äôs biomass is carbon, followed by oxygen, at around 44%. This is also said by Ralph Dubayah (NASA GEDI Mission Principal Investigator). The green power of trees contains carbon not only in the wood but also in their leaves, twigs, branches, roots, and so on.\nUnderstanding the tree‚Äôs above-ground biomass density (AGBD) becomes essential‚Äîit measures the greenery‚Äôs weight, indicative of carbon storage critical for addressing climate change. Trees‚Äô green power, containing roughly half of their biomass as carbon, underscores their immense contribution to our planet‚Äôs health and fight against climate change.\n\n\nSatellite Imageries can help with this matter\nNowadays, as technology and computation power grow, it is possible to rely on big data processing to deal with in-time abundant satellite image data with high-resolution quality.\n\n\n\n\n\n\n\n(a) Satellite above the Earth\n\n\n\n\n\n\n\n(b) Simulation of GEDI lasers collecting data\n\n\n\n\nFigure¬†14: The power of satellite images data. sources from UMD\n\n\nNASA‚Äôs GEDI (Global Ecosystem Dynamics Investigation) mission is a groundbreaking initiative employing advanced laser technology aboard the International Space Station. GEDI precisely measures the height of Earth‚Äôs forests, providing accurate data on L2A canopy top height. This technology revolutionizes forest monitoring by offering detailed information on forest structure and biomass. Traditional methods for measuring canopy height and above-ground biomass density (AGBD) involved ground-based measurements or aerial surveys, which were time-consuming and less comprehensive.\nGEDI‚Äôs innovative approach allows for global coverage, overcoming the limitations of ground-based techniques. By using lasers to measure canopy height, GEDI generates highly accurate 3D maps of forests, offering insights into their structure and carbon storage potential. This satellite-based method significantly enhances our understanding of forests‚Äô carbon storage capacity and their role in mitigating climate change.\nComparing traditional methods to GEDI‚Äôs satellite approach, the latter provides a broader, more consistent, and cost-effective way to assess forests globally. It offers continuous monitoring, whereas ground-based methods often cover limited areas and can be labor-intensive. GEDI‚Äôs data allows scientists to track changes in canopy height and AGBD over time, enabling better forest management strategies crucial for preserving our planet‚Äôs health.\nThis is also an intriguing way for me to figure out a new way to measure carbon stock instead of human power needed and time-consuming. Exploring this innovative method to measure carbon stock is not just about replacing labor-intensive processes; it represents a leap forward in efficiency and accuracy. With GEDI‚Äôs precise data on canopy height and biomass, assessing carbon storage becomes streamlined, allowing for more comprehensive and timely monitoring. This shift from traditional methods to satellite-based technology is not only forward-thinking but also empowers us to better protect and sustain our planet‚Äôs invaluable forests."
  },
  {
    "objectID": "blog/research project story/research-story.html#summary",
    "href": "blog/research project story/research-story.html#summary",
    "title": "Fascinating Malaysia Biodiversity - Research Motivation",
    "section": "Summary",
    "text": "Summary\n\n\n\nFigure¬†15: Observing nature through my binoculars.\n\n\nReflecting on my journey in Malaysia, it‚Äôs the blend of cultures and rich biodiversity that makes this country unique. With influences from East and West‚ÄîChinese, Islamic, Indian, and Western‚Äîthe mix of traditions and natural beauty creates a diverse and fascinating landscape.\nExploring this diverse and nature-rich country has been an eye-opening experience. Nature acts as our best teacher, guiding and teaching us invaluable lessons through its vast biodiversity‚Äîtowering rainforests and captivating wildlife.\nThe topic, ‚ÄúFascinating Malaysia Biodiversity,‚Äù encourages us to eagerly anticipate and cherish each moment. It‚Äôs a reminder to value and protect the delicate balance of life on our planet, nurturing a harmonious relationship between humans and nature. In this connection, we find not just fascination but a deep sense of appreciation and responsibility toward our incredible natural world."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE\n\n\n\ncanopy height\n\n\ncarbon stock\n\n\njavascript\n\n\nmachine learning\n\n\nremote sensing\n\n\ngoogle earth engine\n\n\n\n\n\n\n\nHuang Han Wang\n\n\nJan 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeart Disease Prediction\n\n\n\npython\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\nHuang Han Wang\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html",
    "href": "posts/heart disease prediction/heart-disease.html",
    "title": "Heart Disease Prediction",
    "section": "",
    "text": "Using machine learning methods‚ÄîKNN, Random Forest, and Logistic Regression‚Äîto predict heart disease diagnosis with patient data."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#description",
    "href": "posts/heart disease prediction/heart-disease.html#description",
    "title": "Heart Disease Prediction",
    "section": "Description",
    "text": "Description\nHeart disease is the world‚Äôs leading cause of death, taking around 17.9 million lives yearly, according to the World Health Organization (WHO). Early identification of high-risk individuals is crucial in preventing premature deaths.\nDiagnosing heart disease is complex, often relying on patient symptoms and examinations. Analyzing vast clinical data using data science helps us better understand and predict heart disease occurrence."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#dataset",
    "href": "posts/heart disease prediction/heart-disease.html#dataset",
    "title": "Heart Disease Prediction",
    "section": "Dataset",
    "text": "Dataset\n\nHeart Disease - UCI Machine Learning Repository\n\nFour of the processed files used in this project:\n\nprocessed.switzerland.data\nprocessed.cleveland.data\nprocessed.hungarian.data\nprocessed.va.data"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#project-objectives",
    "href": "posts/heart disease prediction/heart-disease.html#project-objectives",
    "title": "Heart Disease Prediction",
    "section": "Project objectives",
    "text": "Project objectives\n\nAnalyze the heart disease dataset to find common risk factors and patient groups.\nDevelop a heart disease prediction system using KNN, RandomForest, and Logistic Regression algorithms.\nAssess the performance of the created models.\n\n\n\nImport libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#data-loading",
    "href": "posts/heart disease prediction/heart-disease.html#data-loading",
    "title": "Heart Disease Prediction",
    "section": "Data loading",
    "text": "Data loading\n\nThere are 14 attributes in each dataset\nCombined 4 dataset into single dataset\nAll contain same the same columns (without any header, missing values shown as ?)\n\n\n\nCode\ncolumns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\",\n          \"slope\", \"ca\", \"thal\", \"disease\"]\nsdf = pd.read_csv(\"processed.switzerland.csv\", header=None, names=columns, na_values='?')\ncdf = pd.read_csv(\"processed.cleveland.csv\", header=None, names=columns, na_values='?')\nhdf = pd.read_csv(\"processed.hungarian.csv\", header=None, names=columns, na_values='?')\nvdf = pd.read_csv(\"processed.va.csv\", header=None, names=columns, na_values='?')\n\ndf = pd.concat([sdf, cdf, vdf, hdf], ignore_index=True)\ndf.disease = df['disease'].apply(lambda x: 1 if x &gt; 0 else 0)\ndf\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ndisease\n\n\n\n\n0\n32.0\n1.0\n1.0\n95.0\n0.0\nNaN\n0.0\n127.0\n0.0\n0.7\n1.0\nNaN\nNaN\n1\n\n\n1\n34.0\n1.0\n4.0\n115.0\n0.0\nNaN\nNaN\n154.0\n0.0\n0.2\n1.0\nNaN\nNaN\n1\n\n\n2\n35.0\n1.0\n4.0\nNaN\n0.0\nNaN\n0.0\n130.0\n1.0\nNaN\nNaN\nNaN\n7.0\n1\n\n\n3\n36.0\n1.0\n4.0\n110.0\n0.0\nNaN\n0.0\n125.0\n1.0\n1.0\n2.0\nNaN\n6.0\n1\n\n\n4\n38.0\n0.0\n4.0\n105.0\n0.0\nNaN\n0.0\n166.0\n0.0\n2.8\n1.0\nNaN\nNaN\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n915\n52.0\n1.0\n4.0\n160.0\n331.0\n0.0\n0.0\n94.0\n1.0\n2.5\nNaN\nNaN\nNaN\n1\n\n\n916\n54.0\n0.0\n3.0\n130.0\n294.0\n0.0\n1.0\n100.0\n1.0\n0.0\n2.0\nNaN\nNaN\n1\n\n\n917\n56.0\n1.0\n4.0\n155.0\n342.0\n1.0\n0.0\n150.0\n1.0\n3.0\n2.0\nNaN\nNaN\n1\n\n\n918\n58.0\n0.0\n2.0\n180.0\n393.0\n0.0\n0.0\n110.0\n1.0\n1.0\n2.0\nNaN\n7.0\n1\n\n\n919\n65.0\n1.0\n4.0\n130.0\n275.0\n0.0\n1.0\n115.0\n1.0\n1.0\n2.0\nNaN\nNaN\n1\n\n\n\n\n920 rows √ó 14 columns\n\n\n\n\nAttributes\nNumerical attributes\n\nage - age in years, numerical\ntrestbps - resting blood pressure (in mm Hg on admission to the hospital)\nchol - cholesterol in mg/dl\nthalach - maximum heart rate achieved\noldpeak - ST depression induced by exercise relative to rest. ‚ÄòST‚Äô relates to the positions on the electrocardiographic (ECG) plot.\nca - number of major vessels (0-3) colored by flouroscopy. Fluoroscopy is one of the most popular non-invasive coronary artery disease diagnosis. It enables the doctor to see the flow of blood through the coronary arteries in order to evaluate the presence of arterial blockages.\n\nCategorical attributes\n\nsex- sex\n\n1 = male\n0 = female\n\ncp- chest pain type\n\n1 = typical angina\n2 = atypical angina\n3 = non-anginal pain\n4 = asymptomatic\n\nfbs - fasting blood sugar &gt; 120 mg/dl\n\n1 = true\n0 = false\n\nrestecg - resting electrocardiographic (ECG) results\n\n0 = normal\n1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)\n2 = showing probable or definite left ventricular hypertrophy by Estes‚Äô criteria\n\nexang - exercise induced angina. Angina is a type of chest pain caused by reduced blood flow to the heart.\n\n1 = yes\n0 = no\n\nslope - the slope of the peak exercise ST segment. (ECG)\n\n1 = upsloping\n2 = flat\n3 = downsloping\n\nthal - A blood disorder called thalassemia\n\n3 = normal blood flow\n6 = fixed defect (no blood flow in some part of the heart)\n7 = reversable defect (a blood flow is observed but it is not normal)\n\ndisease - refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. In this single dataset, ‚Äò0‚Äô signifies the absence of heart disease, while ‚Äò1‚Äô indicates diagnosed heart disease."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#exploratory-data-analysis-eda",
    "href": "posts/heart disease prediction/heart-disease.html#exploratory-data-analysis-eda",
    "title": "Heart Disease Prediction",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\nNumeric data summary\n\n\nCode\n# Summary statistics for numeric data\nnumeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n\nnumeric_summary = df[numeric_cols].describe().transpose()\nprint(\"Summary Statistics for Numeric Columns:\")\nprint(numeric_summary)\n\n# Distribution plots for numeric attributes\nplt.figure(figsize=(12, 8))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.histplot(df[col].dropna(), kde=True)\n    plt.title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n\n\nSummary Statistics for Numeric Columns:\n          count        mean         std   min    25%    50%    75%    max\nage       920.0   53.510870    9.424685  28.0   47.0   54.0   60.0   77.0\ntrestbps  861.0  132.132404   19.066070   0.0  120.0  130.0  140.0  200.0\nchol      890.0  199.130337  110.780810   0.0  175.0  223.0  268.0  603.0\nthalach   865.0  137.545665   25.926276  60.0  120.0  140.0  157.0  202.0\noldpeak   858.0    0.878788    1.091226  -2.6    0.0    0.5    1.5    6.2\nca        309.0    0.676375    0.935653   0.0    0.0    0.0    1.0    3.0\n\n\n\n\n\n\nAge: The average age in the dataset is approximately 53 years, with most individuals falling between 47 and 60 years old. The youngest person is 28 years old, and the oldest is 77 years old.\nResting Blood Pressure (trestbps): The average resting blood pressure is around 132 mm Hg, with readings typically ranging from 120 to 140 mm Hg. However, there seem to be some unusually low values (minimum at 0) that might need further investigation.\nCholesterol (chol): The average cholesterol level is about 199 mg/dl, with most values spanning between 175 and 268 mg/dl. There are also some entries with cholesterol levels recorded as 0, which might need verification.\nMaximum Heart Rate Achieved (thalach): On average, the maximum heart rate achieved is approximately 138 bpm, with the majority falling between 120 and 157 bpm.\nST Depression (oldpeak): The ST depression induced by exercise relative to rest averages around 0.88. The values range from -2.6 to 6.2.\nNumber of Major Vessels (ca): There are fewer data points available for the number of major vessels. On average, it appears that the dataset has about 0.68 major vessels colored by fluoroscopy, with values ranging from 0 to 3.\n\nGenerate a heatmap to display the correlations between different numeric attributes in heart disease dataset, helping to identify potential relationships or dependencies between these features.\n\n\nCode\n# Compute the correlation matrix\ncorr_matrix = df[numeric_cols].corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)\nplt.title('Correlation Heatmap of Numeric Features')\nplt.show()\n\n\n\n\n\n\n\nCategorical data summary\n\n\nCode\n# Categorical data summary with count plots\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'disease']\n\nplt.figure(figsize=(12, 10))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 3, i)\n    sns.countplot(data=df, x=col)\n    plt.title(f'Count Plot of {col}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCounts for each categorical attribute\n#categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'disease']\n\n#for col in categorical_cols:\n#    counts = df[col].value_counts()\n#    print(f\"Value counts for {col}:\")\n#    print(counts)\n#    print()\n\n\n\nSex: There are 726 instances of the value 1.0 (male) and 194 instances of the value 0.0 (female) in the ‚Äòsex‚Äô column.\nChest Pain Type (cp): 496 instances of type 4 (asymptomatic), 204 instances of type 3 (non-anginal pain), 174 instances of type 2 (atypical angina), and 46 instances of type 1 (typical angina).\nFasting Blood Sugar (fbs): 692 instances with a value of 0.0 (fasting blood sugar &lt;= 120 mg/dl) and 138 instances with a value of 1.0 (fasting blood sugar &gt; 120 mg/dl).\nResting Electrocardiographic Results (restecg): 551 instances with a value of 0.0 (normal), 188 instances with a value of 2.0 (showing probable or definite left ventricular hypertrophy by Estes‚Äô criteria), and 179 instances with a value of 1.0 (having ST-T wave abnormality).\nExercise Induced Angina (exang): 528 instances with a value of 0.0 (no exercise-induced angina) and 337 instances with a value of 1.0 (presence of exercise-induced angina).\nSlope of Peak Exercise ST Segment (slope): 345 instances with a value of 2.0 (a flat slope), 203 instances with a value of 1.0 (an upsloping slope), and 63 instances with a value of 3.0 (a downsloping slope).\nThalassemia (thal): 196 instances with a value of 3.0 (normal blood flow), 192 instances with a value of 7.0 (reversible defect), and 46 instances with a value of 6.0 (fixed defect).\nPresence of Heart Disease (disease): 509 instances with a value of 1 (presence of heart disease) and 411 instances with a value of 0 (absence of heart disease).\n\n\n\nDifferent risk factors affect heart disease\n\n\nCode\n# Display boxplots for each numerical attribute and presence of heart disease \nplt.figure(figsize=(12, 10))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(x='disease', y=col, data=df)\n    plt.title(f'{col.capitalize()} vs Disease')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\n# Grouping by 'disease' and describing numeric attributes\nnumeric_summary = df.groupby('disease')[numeric_cols].describe().transpose()\n\n# Renaming the columns for clarity\nnumeric_summary.columns = ['non-heart disease (disease=0)', 'heart disease (disease=1)']\n\n# Displaying the summary table\nprint(numeric_summary)\n\n\n                non-heart disease (disease=0)  heart disease (disease=1)\nage      count                     411.000000                 509.000000\n         mean                       50.547445                  55.903733\n         std                         9.433700                   8.718959\n         min                        28.000000                  31.000000\n         25%                        43.000000                  51.000000\n         50%                        51.000000                  57.000000\n         75%                        57.000000                  62.000000\n         max                        76.000000                  77.000000\ntrestbps count                     391.000000                 470.000000\n         mean                      129.913043                 133.978723\n         std                        16.869867                  20.552278\n         min                        80.000000                   0.000000\n         25%                       120.000000                 120.000000\n         50%                       130.000000                 130.000000\n         75%                       140.000000                 145.000000\n         max                       190.000000                 200.000000\nchol     count                     392.000000                 498.000000\n         mean                      227.905612                 176.479920\n         std                        75.832760                 127.517611\n         min                         0.000000                   0.000000\n         25%                       199.000000                   0.000000\n         50%                       228.000000                 218.000000\n         75%                       269.000000                 267.750000\n         max                       564.000000                 603.000000\nthalach  count                     391.000000                 474.000000\n         mean                      148.800512                 128.261603\n         std                        23.608692                  24.024193\n         min                        69.000000                  60.000000\n         25%                       134.500000                 112.000000\n         50%                       151.000000                 128.000000\n         75%                       167.500000                 145.000000\n         max                       202.000000                 195.000000\noldpeak  count                     390.000000                 468.000000\n         mean                        0.418205                   1.262607\n         std                         0.715636                   1.197424\n         min                        -1.100000                  -2.600000\n         25%                         0.000000                   0.000000\n         50%                         0.000000                   1.050000\n         75%                         0.800000                   2.000000\n         max                         4.200000                   6.200000\nca       count                     165.000000                 144.000000\n         mean                        0.278788                   1.131944\n         std                         0.640006                   1.012140\n         min                         0.000000                   0.000000\n         25%                         0.000000                   0.000000\n         50%                         0.000000                   1.000000\n         75%                         0.000000                   2.000000\n         max                         3.000000                   3.000000\n\n\nFrom numerical attributes:\n\nAge: Individuals with heart disease tend to be older on average compared to those without. The median age for individuals with heart disease (57 years) is higher than for those without (51 years).\nResting Blood Pressure (trestbps): While the mean resting blood pressure appears slightly higher for individuals with heart disease, there‚Äôs overlap in the interquartile ranges, suggesting variability. However, there seem to be some unusual zero values for blood pressure in the heart disease group that might need further examination.\nCholesterol (chol): There‚Äôs a notable difference in cholesterol levels between the two groups. Individuals without heart disease have higher median cholesterol levels (228 mg/dl) compared to those with heart disease (218 mg/dl). However, there are zero values present in both groups that might need clarification.\nMaximum Heart Rate Achieved (thalach): Those without heart disease generally achieve higher maximum heart rates compared to those with heart disease. The median maximum heart rate for individuals without heart disease (151 bpm) is higher than for those with heart disease (128 bpm).\nST Depression (oldpeak): The magnitude of ST depression induced by exercise relative to rest appears significantly higher in individuals with heart disease. The median ST depression for those with heart disease (1.05) is notably greater compared to those without heart disease (0).\nNumber of Major Vessels (ca): Individuals with heart disease tend to have a higher number of major vessels colored by fluoroscopy. The median number of major vessels for those with heart disease (1) is higher than for those without heart disease (0).\nOutliers: In ‚Äòtrestbps‚Äô and ‚Äòchol‚Äô columns, ‚Äòtrestbps‚Äô displays an outlier with a minimum value of 0, while ‚Äòchol‚Äô has outliers represented by zero values in both categories, which may require further investigation due to their deviation from expected physiological ranges.\n\n\n\nCode\n# Generate stacked barcharts for each categorical value compared to heart disease\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n\nplt.figure(figsize=(15, 12))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 3, i)\n    ct = pd.crosstab(df[col], df['disease'], normalize='index') * 100\n    plot = ct.plot(kind='bar', stacked=True, ax=plt.gca())\n    plt.title(f'{col.capitalize()} vs Disease')\n    plt.ylabel('Percentage')\n    plt.xlabel(col.capitalize())\n    plt.legend(title='Disease', labels=['No Disease', 'Heart Disease'])\n    plt.xticks(rotation=0)\n    \n    for p in plot.patches:\n        width, height = p.get_width(), p.get_height()\n        x, y = p.get_xy() \n        plt.text(x + width / 2, \n                 y + height / 2, \n                 f'{height:.2f}%', \n                 horizontalalignment='center', \n                 verticalalignment='center')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFrom categorical attributes:\n\nSex: Females (sex=0) show a lower percentage of heart disease (25.77%) compared to males (sex=1) with a higher percentage (63.22%).\nChest Pain Type (Cp): Asymptomatic chest pain type (cp=4) displays a significantly higher percentage (79.03%) of heart disease, while typical angina (cp=1) has a lower percentage (43.48%).\nFasting Blood Sugar (Fbs): Individuals with fasting blood sugar greater than 120 mg/dl (fbs=1) tend to have a notably higher percentage (68.12%) of heart disease compared to those with lower fasting blood sugar.\nExercise Induced Angina (Exang): Individuals experiencing exercise-induced angina (exang=1) demonstrate a notably higher percentage (83.68%) of heart disease compared to those without it.\nThalassemia (Thal): Reversible defect thalassemia (thal=7) shows a higher percentage (80.21%) of heart disease compared to other types of thalassemia.\n\n\n\nSummary of EDA\nInsights from the Exploratory Data Analysis:\n\nAge Factor: Individuals diagnosed with heart disease tend to be older than those without the condition, suggesting age as a contributing risk factor for heart disease.\nChest Pain Types: Asymptomatic chest pain (cp=4) presents a notably higher prevalence of heart disease, while typical angina (cp=1) exhibits a lower association with heart conditions.\nCholesterol and Heart Disease: While cholesterol levels vary, individuals without heart disease often exhibit higher median cholesterol levels than those with the condition, although zero values in both groups require further validation.\nExercise-Induced Angina: The presence of exercise-induced angina (exang=1) demonstrates a significantly higher likelihood of heart disease compared to its absence (exang=0).\nGender Disparity: Males (sex=1) tend to show a higher percentage of heart disease cases (63.22%) compared to females (sex=0) with a lower prevalence (25.77%).\nFasting Blood Sugar (FBS): Elevated fasting blood sugar (&gt;120 mg/dl) correlates with a higher percentage of heart disease instances (68.12%) compared to lower levels.\nResting Blood Pressure and Heart Disease: Resting blood pressure, though variable, shows a trend toward higher values for heart disease cases, though outliers (0 values) necessitate further investigation."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#data-preparation",
    "href": "posts/heart disease prediction/heart-disease.html#data-preparation",
    "title": "Heart Disease Prediction",
    "section": "Data preparation",
    "text": "Data preparation\n\nFind incorrect values\n\nCholesterol (chol) contains lots of zero values.\nST Depression (oldpeak) gets some negetive values.\nResting Blood Pressure (trestbps) has one zero value.\n\nCount and plot those incorrect (missing) values.\n\n\nCode\n# Create a copy of the dataframe to handle missing values\ndf_missing = df.copy()\n\n# Replace incorrect values with NaN in specific columns\ndf_missing['chol'] = df['chol'].replace({0: np.nan})\ndf_missing['trestbps'] = df['trestbps'].replace({0: np.nan})\ndf_missing.loc[df['oldpeak'] &lt; 0, 'oldpeak'] = np.nan\n\n# Calculate missing value counts and percentages\nna_values_percent = df_missing.isna().sum().sort_values(ascending=False) \\\n    .apply(lambda x: (x, round(x / len(df_missing) * 100, 2)))\n\n# Plotting the missing value percentages\nna_values_percent.apply(lambda x: x[1]).plot.bar(title='Percentage of Missing Values in each Column')\nplt.xlabel('Columns')\nplt.ylabel('Percentage of Missing Values')\n\n# Annotating the bars with values\nfor i, val in enumerate(na_values_percent.apply(lambda x: x[1])):\n    plt.text(i, val + 1, f\"{val}%\", ha='center', va='bottom')\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Display the count and percentage of missing/incorrect values\nprint(na_values_percent)\n\n\n\n\n\nca          (611, 66.41)\nthal        (486, 52.83)\nslope       (309, 33.59)\nchol        (202, 21.96)\nfbs           (90, 9.78)\noldpeak       (74, 8.04)\ntrestbps      (60, 6.52)\nthalach       (55, 5.98)\nexang         (55, 5.98)\nrestecg        (2, 0.22)\ncp              (0, 0.0)\nsex             (0, 0.0)\nage             (0, 0.0)\ndisease         (0, 0.0)\ndtype: object\n\n\n\n\nData cleaning\nSet incorrect values to NaN and find out duplicate values to remove.\n\n\nCode\n# Create a copy of the dataframe to handle missing values and duplicates\ndf_clean = df.copy()\n\n# Replace incorrect values with NaN in specific columns\ndf_clean['chol'] = df['chol'].replace({0: np.nan})\ndf_clean['trestbps'] = df['trestbps'].replace({0: np.nan})\ndf_clean.loc[df['oldpeak'] &lt; 0, 'oldpeak'] = np.nan\n\n# Display the count of missing values before cleaning\nprint(\"Missing Values Before Cleaning:\")\nprint(df.isnull().sum())\n\n# Remove duplicate rows\nduplicate_rows = df_clean[df_clean.duplicated()]\nprint(\"\\nDuplicate Rows:\")\nprint(duplicate_rows)\n\n# Drop duplicate rows\ndf_clean.drop_duplicates(inplace=True)\n\n# Display the count of missing values after cleaning\nprint(\"\\nMissing Values After Cleaning:\")\nprint(df_clean.isnull().sum())\n\n\nMissing Values Before Cleaning:\nage           0\nsex           0\ncp            0\ntrestbps     59\nchol         30\nfbs          90\nrestecg       2\nthalach      55\nexang        55\noldpeak      62\nslope       309\nca          611\nthal        486\ndisease       0\ndtype: int64\n\nDuplicate Rows:\n      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n613  58.0  1.0  3.0     150.0  219.0  0.0      1.0    118.0    1.0      0.0   \n728  49.0  0.0  2.0     110.0    NaN  0.0      0.0    160.0    0.0      0.0   \n\n     slope  ca  thal  disease  \n613    NaN NaN   NaN        1  \n728    NaN NaN   NaN        0  \n\nMissing Values After Cleaning:\nage           0\nsex           0\ncp            0\ntrestbps     60\nchol        201\nfbs          90\nrestecg       2\nthalach      55\nexang        55\noldpeak      74\nslope       307\nca          609\nthal        484\ndisease       0\ndtype: int64"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#feature-engineering",
    "href": "posts/heart disease prediction/heart-disease.html#feature-engineering",
    "title": "Heart Disease Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nData imputation\n\n\nCode\n# Columns for mode imputation\nmode_cols = ['restecg', 'exang', 'fbs', 'slope', 'thal']\n\n# Columns for median imputation\nmedian_cols = ['oldpeak', 'trestbps', 'thalach', 'chol', 'ca']\n\n# Impute missing values with mode for mode_cols\nmode_imputer = SimpleImputer(strategy='most_frequent')\ndf_clean[mode_cols] = mode_imputer.fit_transform(df_clean[mode_cols])\n\n# Impute missing values with median for median_cols\nmedian_imputer = SimpleImputer(strategy='median')\ndf_clean[median_cols] = median_imputer.fit_transform(df_clean[median_cols])\n\n# Verify the imputed values\nprint(\"Missing Values After Imputation:\")\nprint(df_clean.isnull().sum())\n\n\nMissing Values After Imputation:\nage         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ndisease     0\ndtype: int64\n\n\n\n\nCode\n# Check the number of rows after data imputation\nnum_rows_after_imputation = df_clean.shape[0]\nprint(f\"Number of rows after data imputation: {num_rows_after_imputation}\")\n\n\nNumber of rows after data imputation: 918\n\n\nThe total number of rows before the data split was 920. Since only two duplicate rows were removed, the remaining rows became 918.\n\n\nOne-hot encoding\nIt‚Äôs important to apply One-Hot Encoding before splitting data into training and testing sets. Doing this ensures that each set is independently encoded, preventing any mixing of information between them. This separation is important because it helps the model learn and make predictions correctly without being influenced by how the data was divided.\n\n\nCode\n# Categorical columns for one-hot encoding\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n\n# Apply one-hot encoding\ndf_encoded = pd.get_dummies(df_clean, columns=categorical_cols)\n\n# Display the first few rows to verify the encoding\nprint(df_encoded.head())\n\n\n    age  trestbps   chol  thalach  oldpeak   ca  disease  sex_0.0  sex_1.0  \\\n0  32.0      95.0  240.0    127.0      0.7  0.0        1    False     True   \n1  34.0     115.0  240.0    154.0      0.2  0.0        1    False     True   \n2  35.0     130.0  240.0    130.0      0.5  0.0        1    False     True   \n3  36.0     110.0  240.0    125.0      1.0  0.0        1    False     True   \n4  38.0     105.0  240.0    166.0      2.8  0.0        1     True    False   \n\n   cp_1.0  ...  restecg_1.0  restecg_2.0  exang_0.0  exang_1.0  slope_1.0  \\\n0    True  ...        False        False       True      False       True   \n1   False  ...        False        False       True      False       True   \n2   False  ...        False        False      False       True      False   \n3   False  ...        False        False      False       True      False   \n4   False  ...        False        False       True      False       True   \n\n   slope_2.0  slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n0      False      False      True     False     False  \n1      False      False      True     False     False  \n2       True      False     False     False      True  \n3       True      False     False      True     False  \n4      False      False      True     False     False  \n\n[5 rows x 26 columns]\n\n\n\n\nFeature scaling & Split dataset\nNormalization / Standardization While ML algorithms like Random Forest, linear regression, logistic regression, and neural networks don‚Äôt require scaling, it‚Äôs crucial for Distance algorithms such as KNN, K-means, and SVM.\nTo accommodate this, I‚Äôll split the dataset: one portion for KNN and logistic regression, where normalization is employed, and another for RF modeling, where scaling isn‚Äôt necessary.\n70% for Train & 30% for Test\n\n\nCode\n# Select numerical attributes for scaling\nnumerical_cols = ['age', 'oldpeak', 'chol', 'thalach', 'trestbps']\n\n# Extract features and target variable\nX = df_encoded.drop('disease', axis=1)\ny = df_encoded['disease']\n\n# Splitting the dataset into train and test sets (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Separate the numerical columns for scaling\nX_train_numeric = X_train[numerical_cols]\nX_test_numeric = X_test[numerical_cols]\n\n# Feature scaling for numerical attributes for KNN and Logistic Regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_numeric)\nX_test_scaled = scaler.transform(X_test_numeric)\n\n# Replace the scaled numerical columns in the train and test sets for KNN and Logistic Regression\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_cols, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_cols, index=X_test.index)\n\nX_train_knn_logreg = X_train.copy()\nX_test_knn_logreg = X_test.copy()\n\nX_train_knn_logreg[numerical_cols] = X_train_scaled\nX_test_knn_logreg[numerical_cols] = X_test_scaled"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#modeling",
    "href": "posts/heart disease prediction/heart-disease.html#modeling",
    "title": "Heart Disease Prediction",
    "section": "Modeling",
    "text": "Modeling\n\nK-Nearest Neighbors (KNN)\n\n\nCode\nknn = KNeighborsClassifier()\nknn.fit(X_train_knn_logreg, y_train)\ny_pred_knn = knn.predict(X_test_knn_logreg)\n\n\n/Users/harrywang/.virtualenvs/r-reticulate/lib/python3.9/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.9 is maybe too old for this OS.\n  warnings.warn(\n\n\n\n\nLogistic Regression\n\n\nCode\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_knn_logreg, y_train)\ny_pred_log_reg = log_reg.predict(X_test_knn_logreg)\n\n\n\n\nRandom Forest (data used without scaling)\n\n\nCode\nrf = RandomForestClassifier(random_state=37)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)"
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#evaluation-models-performance",
    "href": "posts/heart disease prediction/heart-disease.html#evaluation-models-performance",
    "title": "Heart Disease Prediction",
    "section": "Evaluation models performance",
    "text": "Evaluation models performance\n\n\nCode\nmodels = {\n    \"KNN\": (y_test, y_pred_knn),\n    \"Logistic Regression\": (y_test, y_pred_log_reg),\n    \"Random Forest\": (y_test, y_pred_rf)\n}\n\nfor model_name, (true, pred) in models.items():\n    accuracy = accuracy_score(true, pred)\n    precision = precision_score(true, pred)\n    recall = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    auc = roc_auc_score(true, pred)\n    \n    print(f\"Metrics for {model_name}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"AUC Score: {auc:.4f}\")\n    print(\"\\n\")\n\n\nMetrics for KNN:\nAccuracy: 0.8043\nPrecision: 0.7829\nRecall: 0.8954\nF1 Score: 0.8354\nAUC Score: 0.7932\n\n\nMetrics for Logistic Regression:\nAccuracy: 0.7935\nPrecision: 0.7824\nRecall: 0.8693\nF1 Score: 0.8235\nAUC Score: 0.7842\n\n\nMetrics for Random Forest:\nAccuracy: 0.8152\nPrecision: 0.8072\nRecall: 0.8758\nF1 Score: 0.8401\nAUC Score: 0.8078\n\n\n\n\n\n\nCode\n# Define a function to plot confusion matrix\ndef plot_confusion(y_true, y_pred, title):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n# Plot confusion matrix for KNN\nplot_confusion(y_test, y_pred_knn, 'Confusion Matrix - KNN')\n\n# Plot confusion matrix for Logistic Regression\nplot_confusion(y_test, y_pred_log_reg, 'Confusion Matrix - Logistic Regression')\n\n# Plot confusion matrix for Random Forest\nplot_confusion(y_test, y_pred_rf, 'Confusion Matrix - Random Forest')\n\n\n\n\n\n\n\n\n\n\n\nThe table display the model evaluation comparison.\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nRecall\nPrecision\nF1 Score\nAUC Score\n\n\n\n\nKNN\n0.8043\n0.8954\n0.7829\n0.8354\n0.7932\n\n\nLogistic Regression\n0.7935\n0.8693\n0.7824\n0.8235\n0.7842\n\n\nRandom Forest\n0.8152\n0.8758\n0.8072\n0.8401\n0.8078\n\n\n\n\nAccuracy Comparison: Random Forest achieved the highest accuracy (81.52%), outperforming both KNN (80.43%) and Logistic Regression (79.35%).\nPrecision and Recall: Random Forest and KNN showed similar recall rates of 87.58%, signifying their proficiency in capturing positive cases. However, Random Forest showcased superior precision at 80.72% compared to KNN‚Äôs 78.29% and Logistic Regression‚Äôs 78.24%. This highlights Random Forest‚Äôs better balance between precision and recall in correctly identifying positive cases while minimizing false positives.\nF1 Score: Random Forest yielded the highest F1 score (84.01%), signifying a balanced performance between precision and recall, followed by KNN (83.54%) and Logistic Regression (82.35%).\nAUC Score: Random Forest had the highest AUC score (80.78%), indicating its capability to distinguish between classes better than KNN (79.32%) and Logistic Regression (78.42%).\nRandom Forest Model: Overall, Random Forest demonstrated superior performance across multiple metrics, showcasing its robustness in predicting heart disease compared to the other models."
  },
  {
    "objectID": "posts/heart disease prediction/heart-disease.html#conclusion",
    "href": "posts/heart disease prediction/heart-disease.html#conclusion",
    "title": "Heart Disease Prediction",
    "section": "Conclusion",
    "text": "Conclusion\n\nEmploying KNN, Logistic Regression, and Random Forest algorithms, models were constructed to predict heart disease with accuracies ranging between 79.35% and 81.52%. Notably, the Random Forest model emerged as the most effective model, exhibiting an accuracy of 81.52% and an F1 score of 84.01%, showcasing its robustness in identifying heart disease cases.\nThrough comprehensive exploratory data analysis (EDA), important insights were revealed. Factors such as age, chest pain type, cholesterol levels, and exercise-induced angina showed notable correlations with the presence of heart disease. Visualizing these relationships aided in understanding risk factors associated with heart conditions.\nThe data preparation phase involved handling missing values, erroneous entries, and duplicates. Feature engineering encompassed imputation of missing values, one-hot encoding for categorical variables, and scaling for suitable algorithms, ensuring the data was ready for modeling.\nThe significance of various attributes in predicting heart disease was evident. Numeric attributes like age, cholesterol levels, and ST depression were influential, while categorical features such as chest pain type and thalassemia also played crucial roles. Random Forest, exhibiting superior performance, emphasized the importance of these features in predicting heart disease.\nThe project‚Äôs success in predictive modeling offers potential clinical implications. These models can aid healthcare practitioners by providing insights into patient risk factors for heart disease. However, further refinement through hyperparameter tuning and larger datasets could enhance predictive accuracy, paving the way for more reliable clinical decision support systems."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "",
    "text": "This research motivation comes from an awe of the beauty of nature. To dive into how unique nature is and my research motivation, click here Fascinating Malaysia Biodiversity - Research Motivation."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#description",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#description",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Description",
    "text": "Description\nThis project uses satellite imagery on Google Earth Engine (GEE) to predict canopy height and estimate carbon content in the University of Malaya (Kuala Lumpur, Malaysia) area."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#overview",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#overview",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Overview",
    "text": "Overview\nTo give a brief overview of this research‚Äôs scope, I‚Äôve created an infographic to showcase this project‚Äôs objectives, impacts, results, and demos.\n\n\n\nInfographic for the Research"
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#setup-and-installation",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#setup-and-installation",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Setup and Installation",
    "text": "Setup and Installation\n\nPrerequisites: Obtain access to Google Earth Engine Code Editor and have a GEE account. If not, please register here.\nInstallation: No installation required; access GEE platform and import necessary scripts."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#usage",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#usage",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Usage",
    "text": "Usage\n\nExample snippets to execute canopy height prediction and carbon estimation using GEE‚Äôs JavaScript.\nInstructions on how to run the scripts within the GEE platform."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#data-sources",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#data-sources",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Data Sources",
    "text": "Data Sources\n\nSentinel-1 (C-band synthetic aperture radar SAR):\nSentinel-1 is a space mission by the European Union (EU) and carried out by ESA under the Copernicus Program.\n\nVV and VH bands capture signals reflected from the Earth‚Äôs surface, including vegetation and terrain.\nUse VV and VH as predictor variables. These bands offer radar backscatter values that can be related to vegetation structure, including canopy height, as validation with GEDI Data.\n\nSentinel-2:\nSentinel-2 is a European multi-spectral imaging mission with high resolution.\n\nIncluded 12 spectral bands with resolutions of 10m, 20m, and 60m.\nPrimary objective: monitoring variability in land surface conditions.\nSelect bands ‚ÄòB2‚Äô, ‚ÄòB3‚Äô, ‚ÄòB4‚Äô, ‚ÄòB5‚Äô, ‚ÄòB6‚Äô, ‚ÄòB7‚Äô, ‚ÄòB8‚Äô, ‚ÄòB11‚Äô, ‚ÄòB12‚Äô as predictor variables. ‚Üí Manual feature selection\nTake all the 12 bands from ‚ÄòB1‚Äô to ‚ÄòB12‚Äô as independent variables. ‚Üí Automatic feature selection\n\nNASA SRTM Digital Elevation 30m:\nSRTM data is used to construct a global digital elevation model.\n\nSRTM was conducted aboard the space shuttle Endeavour from February 11-22, 2000.\nExtract ‚ÄúSlope‚Äù and ‚ÄúElevation‚Äù as predictor variables.\n\nEuropean Space Agency (ESA) WorldCover 10m:\nThe ESA dataset was launched since there is a massive need for accurate, timely, and high-resolution information on land cover, land use, and earth surface change.\n\nUse value 10 (Tree Cover only) as a predictor variable.\n\nGEDI L2A Raster Canopy Top Height:\nGEDI Light Detection and Ranging (Lidar) is an active remote sensing technology that uses a laser pulse to measure distance.\n\nPrimarily used for measuring tree height, aboveground biomass, as well as leaf area index.\nUsing ‚Äòrh98‚Äô (relative height metrics at 98%) to be the sampling point.\n\nGEDI L4B Gridded Aboveground Biomass Density:\nThis dataset encompasses gridded information on aboveground biomass density derived from GEDI observations. These data provide insights into vegetation growth and distribution, offering valuable information for understanding global vegetation health and the impacts of climate change.\n\nUsing ‚ÄòMU‚Äô (mean aboveground biomass density, which estimated mean AGBD for the 1 km grid cell, including forest and non-forest.) to be the sampling point."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#project-model-structure",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#project-model-structure",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Project Model Structure",
    "text": "Project Model Structure\n\n\n\nResearch Model Structure\n\n\nThis model represents an example of a canopy height prediction workflow. For the AGBD prediction modeling and PCA (Principal Component Analysis), I‚Äôve organized them into separate folders on my GitHub for easy reference.\n\n\n\nResearch Study Area (University of Malaya) from Sentinel-2 imageries\n\n\nThe research project focuses on the University of Malaya as its study area, leveraging the Sentinel-2 dataset spanning from April 1st, 2021, to June 30th, 2021. Situated at the border of Kuala Lumpur and Petaling Jaya (Selangor), the university campus is nestled within a forested landscape. Notably, within this campus lies Rimba Ilmu (The Forest of Knowledge), a reserved forest established in 1974, covering approximately 80 hectares and hosting remarkable collections. Rimba Ilmu serves a pivotal role, utilizing its extensive resources to educate the public and students about tropical plant life, ecology, and conservation, thus fostering awareness about these critical aspects."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#minimal-reproducible-example-javascript-code",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#minimal-reproducible-example-javascript-code",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Minimal Reproducible Example (JavaScript Code)",
    "text": "Minimal Reproducible Example (JavaScript Code)\n\n\n\n\n\n\nNote\n\n\n\nImportant Note: This example code illustrates the process of conducting canopy height regression modeling utilizing the Random Forest model along with manual feature selection.\n\n\n\nSection 1: Boundary Selection\n// Choose boundary on GEE\nvar boundary = ee.Geometry.Polygon(/* Coordinates */);\n\n// Create a polygon feature\nvar polygon = ee.Feature(ee.Geometry.Polygon(boundary.coordinates()));\n\n// Create a feature collection from the polygon\nvar boundary_feature = ee.FeatureCollection([polygon]);\n\n// Print the feature collection\nprint('Boundary:', boundary_feature);\n\n\nSection 2: Importing Datasets (Sentinel-1, Sentinel-2, SRTM, ESA, GEDI L2A)\n// Load Sentinel-1 for the non-rainy season.\nvar S1_PRS = ee.ImageCollection('COPERNICUS/S1_GRD')\n    .filterDate('2021-04-01', '2021-06-30')\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n    .filter(ee.Filter.bounds(boundary));\n\n// Load Sentinel-2 spectral reflectance data.\nvar s2 = ee.ImageCollection('COPERNICUS/S2_SR');\n\n// Load SRTM\nvar SRTM = ee.Image(\"USGS/SRTMGL1_003\");\n// Clip Elevation\nvar elevation = SRTM.clip(boundary);\n\n// Load ESA World cover data\nvar dataset = ee.ImageCollection(\"ESA/WorldCover/v100\").first();\n\n// Clip the land cover to the boundary\nvar ESA_LC_2020 = dataset.clip(boundary);\n\n// Extract forest areas from the land cover value = 10\nvar forest_mask = ESA_LC_2020.updateMask(\n  ESA_LC_2020.eq(10) // Only keep pixels where class equals 2\n);\n\n// Clip land cover to the boundary and visualize forests\n// Display forests only from ESA imagery.\nvar visualization = {bands: ['Map'],};\n\nMap.addLayer(forest_mask, visualization, \"Trees\");\n\n// Import the \"GEDI02_A_002_MONTHLY\" dataset\nvar dataset = ee.ImageCollection('LARSE/GEDI/GEDI02_A_002_MONTHLY')\n                  .map(qualityMask)\n                  .select('rh98').filterBounds(boundary);\n                  \n// Create a palette to visaulize the dataset\nvar gediVis = {\n  min: 1,\n  max: 60,\n  palette: 'darkred, red, orange, green, darkgreen', \n};\n\n// Set the map center and visualize the dataset\nMap.setCenter(101.6556468417507, 3.127386383953448, 13);\nMap.addLayer(dataset, gediVis, 'rh98');\n\n\nSection 3: Preparing Data\n// Prepare inter-quartile range (IQR)\nvar S1_PRS_pc = S1_PRS.reduce(ee.Reducer.percentile([25,50,75]));\n\n// Convert to natural units (linear units, which can be averaged)\nvar S1_PRS_pc = ee.Image(10).pow(S1_PRS_pc.divide(10));\n\nvar S1_PRS_pc_Feats = S1_PRS_pc.select(['VH_p50', 'VV_p50']).clip(boundary);\n\n// Reproject to MY zone\nvar S1_PRS_pc_Feats = S1_PRS_pc_Feats.reproject({crs: 'EPSG:32647', scale:30});\n\n// Check projection information\nprint('Projection, crs, and crs_transform:', S1_PRS_pc_Feats.projection());\n\n// Also, Sentinel-2 and SRTM images have to be reprojected.\n//Reproject to MY zone\nvar S2_composite = composite.median().reproject({crs: 'EPSG:32647', scale:30});\n\n// Check projection information\nprint('S2_Projection, crs, and crs_transform:', S2_composite.projection());\n\n// Reproject 'elevation' to MY zone\nvar elevation = elevation.reproject({crs: 'EPSG:32647', scale:30});\n\n// Check projection information\nprint('elevation_Projection, crs, and crs_transform:', elevation.projection());\n\n// Derive slope from SRTM\nvar slope = ee.Terrain.slope(SRTM).clip(boundary);\n\n// Reproject 'slope' to MY zone\nvar slope = slope.reproject({crs: 'EPSG:32647', scale:30});\n\n// Check projection information\nprint('slope_Projection, crs, and crs_transform:', slope.projection());\n\n\nSection 4: Data Processing and Visualization\n// Calculate IQR for the VV and VH polarizations\n// Calculate IQR for the VV polarization\nvar PRS_VV_iqr = S1_PRS_pc_Feats\n  .addBands((S1_PRS_pc.select('VV_p75')\n  .subtract(S1_PRS_pc.select('VV_p25')))\n  .rename('VV_iqr'));\n  \n// Calculate IQR for the VH polarization\nvar PRS_VH_iqr = S1_PRS_pc_Feats\n  .addBands((S1_PRS_pc.select('VH_p75')\n  .subtract(S1_PRS_pc.select('VH_p25')))\n  .rename('VH_iqr'));\n  \n// Print the image to the console\nprint('Post-rainy Season VV IQR', PRS_VV_iqr);\n\n// Print the image to the console\nprint('Post-rainy Season VH IQR', PRS_VH_iqr);\n\n// Display S1 IQR imagery\nMap.addLayer(PRS_VV_iqr.clip(boundary), {'bands': 'VV_iqr', min: 0, max: 0.1}, 'Sentinel1 IW VV');\nMap.addLayer(PRS_VH_iqr.clip(boundary), {'bands': 'VH_iqr', min: 0, max: 0.1}, 'Sentinel1 IW VH');\n\n// Create masks, filter clouds from Sentinel-2, and prepare composite imagery\n// Create a function to mask clouds using the Sentinel-2 QA band.\nfunction maskS2clouds(image) {\n  var qa = image.select('QA60');\n  \n  //Bits 10 and 11 are clouds and cirrus, respectively.\n  var cloudBitMask = ee.Number(2).pow(10).int();\n  var cirrusBitMask = ee.Number(2).pow(11).int();\n  \n  //Both flags should be set to zero, indicating clear conditions.\n  var mask = qa.bitwiseAnd(cloudBitMask).eq(0).and(\n             qa.bitwiseAnd(cirrusBitMask).eq(0));\n  \n  //Return the masked and scaled data.\n  return image.updateMask(mask).divide(10000);\n}\n\n// Filter clouds from Sentinel-2 for a given period.\nvar composite = s2.filterDate('2021-04-01', '2021-06-30')\n                  // Pre-filter to get less cloudy granules.\n                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n                  .map(maskS2clouds)\n                  .select('B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12');\n\n// Display a composite S2 imagery\nMap.addLayer(S2_composite.clip(boundary), {bands: ['B11', 'B8', 'B3'], min: 0, max: 0.3}, 'S2_composite');\n\n\nSection 5: Data Fusion and Model Training\n// Merge predictor variables\nvar mergedCollection = S2_composite\n  .addBands(PRS_VV_iqr)\n  .addBands(PRS_VH_iqr)\n  .addBands(elevation)\n  .addBands(slope)\n  .addBands(forest_mask);\n  \n// Clip to the output image to Uni Malaya study area boundary.\nvar clippedmergedCollection = mergedCollection.clipToCollection(boundary_feature);\nprint('clippedmergedCollection: ', clippedmergedCollection);\n\n// Bands to include in the classification\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12', 'VV_iqr', 'VH_iqr', 'elevation', 'slope', 'Map'];\n\n// Prepare training dataset from GEDI\n// GEDI's Level 2A Geolocated Elevation and Height Metrics Product (GEDI02_A) is primarily composed of\n// 100 Relative Height (RH) metrics, which collectively describe the waveform collected by GEDI.\n// The original GEDI02_A product is a table of point with a spatial resolution (average footprint) of 25 meters.\n// Sample 10,000 points from the dataset (Depending on the area in which GEDI received the points more or less)\nvar points = mosaic.sample({\n   region: boundary,\n   scale: scale,\n   numPixels: 10000,\n   projection: projection,\n   seed:66,\n   geometries: true});\n  \n// Check the number of the training points\nprint('Size: ', points.size());\nprint(points.limit(10));\n\n// Display the training points\nMap.addLayer(points, {}, 'Sampling points');\n\n// Add a random column (named random) and specify seed value for repeatability.\nvar datawithColumn = points.randomColumn('random', 27);\n\n// Seperate 70% for training, 30% for validation\nvar split = 0.7;\nvar trainingData = datawithColumn.filter(ee.Filter.lt('random', split));\n\n// Print the training data\nprint('training data', trainingData);\n\nvar validationData = datawithColumn.filter(ee.Filter.gte('random', split));\n\n// Print the testing (validation) data\nprint('validation data', validationData);\n\n// Train a random forest regression model\n// Collect training data\nvar training = clippedmergedCollection.select(bands).sampleRegions({\n  collection: trainingData,\n  properties: ['rh98'],\n  scale: 30 // Need to change the scale of training data to avoid the 'out of memory' problem\n});\n\n// Train a random forest classifier for regression\nvar classifier = ee.Classifier.smileRandomForest({numberOfTrees:50, seed:3})    // 50 trees\n  .setOutputMode('REGRESSION')\n  .train({\n    features: training,\n    classProperty: 'rh98',\n    inputProperties: bands\n  });\n\n// Run the classification and clip it to the boundary\nvar regression = clippedmergedCollection.select(bands)\n  .classify(classifier, 'predicted')\n  .clip(boundary);\n\nprint(\"regression: \", regression);\n\n\nSection 6: Model Evaluation and Export\n// Visualize regression output and define a palette\nvar palette = ['#f7fcb9','#addd8e','#31a354'];\n\n// Display the regression output.\n// Get dictionaries of min & max predicted value\nvar regressionMin = (regression.reduceRegion({\n   reducer: ee.Reducer.min(),\n   scale: 30,\n   crs: 'EPSG:32647',\n   geometry: boundary,\n   bestEffort: true,\n   tileScale: 5\n}));\nvar regressionMax = (regression.reduceRegion({\n   reducer: ee.Reducer.max(),\n   scale: 30,\n   crs: 'EPSG:32647',\n   geometry: boundary,\n   bestEffort: true,\n   tileScale: 5\n}));\nvar regressionAvg = (regression.reduceRegion({\n   reducer: ee.Reducer.mean(),\n   scale: 30,\n   crs: 'EPSG:32647',\n   geometry: boundary,\n   bestEffort: true,\n   tileScale: 5\n}));  \n\n// Add to map\nvar viz = {palette: palette, min:regressionMin.getNumber('predicted').getInfo(), max:regressionMax.getNumber('predicted').getInfo()};\nMap.addLayer(regression, viz, 'Regression');\n\n// Check model performance\nprint(\"Regression Min & Max: \", regressionMin.getNumber('predicted').getInfo(), regressionMax.getNumber('predicted').getInfo());\nprint(\"Regression Average: \", regressionAvg.getNumber('predicted').getInfo());\n\n// Create model assessment statistics\n// Get predicted regression points in same location as training data\nvar predictedTraining = regression.sampleRegions({collection: trainingData, geometries: true});\n\n// Seperate the observed (rh98_GEDI) & predicted (regression) properties\nvar sampleTraining = predictedTraining.select(['rh98', 'predicted']);\n\n// Compute Root Mean Squared Error (RMSE)\n// Get array of observation and prediction values\nvar observationTraining = ee.Array(sampleTraining.aggregate_array('rh98'));\n\nvar predictionTraining = ee.Array(sampleTraining.aggregate_array('predicted'));\n\n// Compute residuals\nvar residualsTraining = observationTraining.subtract(predictionTraining);\n\n// Compute RMSE (Training) with equation and print the result\nvar rmseTraining = residualsTraining.pow(2).reduce('mean', [0]).sqrt();\nprint('Training RMSE', rmseTraining);\n\n//Perform validation\n// Get predicted regression points in same location as validation data\nvar predictedValidation = regression.sampleRegions({collection: validationData, geometries: true});\n\n// Seperate the observed (rh98_GEDI) & predicted (regression) properties\nvar sampleValidation = predictedValidation.select(['rh98', 'predicted']);\n\n// Compute RMSE (Validation)\n// Get array of observation and prediction values\nvar observationValidation = ee.Array(sampleValidation.aggregate_array('rh98'));\n\nvar predictionValidation = ee.Array(sampleValidation.aggregate_array('predicted'));\n\n// Compute residuals\nvar residualsValidation = observationValidation.subtract(predictionValidation);\n\n// Compute RMSE with equation and print the result\nvar rmseValidation = residualsValidation.pow(2).reduce('mean', [0]).sqrt();\nprint('Validation RMSE', rmseValidation);\n\n// Export the regression output\nExport.image.toDrive({\n  image:regression,\n  description: 'UM_TCH_GEDI_2021',  //Please change your description here.\n  scale: 20,\n  crs: 'EPSG:32647',\n  maxPixels: 6756353855,\n  region: boundary\n});"
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#results",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#results",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Results",
    "text": "Results\n\nCanopy Height Prediction Results\nTable 1 showcases the outcomes of various machine learning models in predicting canopy height based on the satellite imagery and geographical data utilized in this project:\n\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Model\nMin. Canopy Height (m)\nAvg. Canopy Height (m)\nMax. Canopy Height (m)\n\n\n\n\n1\nRandom Forest (RF)\n7.435\n13.69\n20.847\n\n\n2\nRF with PCA\n5.81\n11.714\n23.469\n\n\n3\nGradient Boost Tree Regression\n1.466\n9.182\n22.049\n\n\n4\nGBTR with PCA\n4.457\n10.437\n20.889\n\n\n5\nSupport Vector Machine (SVM)\n11.087\n12.368\n14.058\n\n\n6\nSVM with PCA\n7.809\n9.318\n12.036\n\n\n\nThese results demonstrate the variation in minimum, average, and maximum canopy heights predicted by different machine learning models, highlighting each model‚Äôs potential to effectively estimate canopy height.\n\n\nModel Assessment Metrics\nTable 2 represents the assessment metrics of the machine learning models used for canopy height prediction:\n\n\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Model\nR¬≤ Training (70%)\nR¬≤ Testing (30%)\nRMSE (m) Training\nRMSE (m) Testing\n\n\n\n\n1\nRandom Forest (RF)\n0.655\n0.355\n7.954\n5.523\n\n\n2\nRF with PCA\n0.731\n0.721\n6.814\n4.05\n\n\n3\nGradient Boost Tree Regression\n0.389\n0.377\n8.654\n6.686\n\n\n4\nGBTR with PCA\n0.577\n0.685\n7.716\n4.196\n\n\n5\nSupport Vector Machine (SVM)\n0.604\n0.193\n9.905\n7.764\n\n\n6\nSVM with PCA\n0.169\n0.48\n11.085\n8.511\n\n\n\nThese metrics gauge the performance of each model in predicting canopy height, providing insights into their training and testing R¬≤ values along with the Root Mean Squared Error (RMSE). A higher R¬≤ value closer to 1 and lower RMSE values indicate better predictive accuracy.\nDescribe these tables to highlight the significance of each model‚Äôs performance metrics in predicting canopy height and assessing the model‚Äôs accuracy using R¬≤ and RMSE values."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#carbon-stock-estimation",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#carbon-stock-estimation",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Carbon Stock Estimation",
    "text": "Carbon Stock Estimation\n\nAboveground Biomass Density (ABGD) Calculation Results\nUsing the GEDI L4B dataset and employing the Random Forest modeling method, the aboveground biomass prediction results in the University of Malaya are as follows:\n\nTable 3 Aboveground Biomass Prediction Results\n\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Model\nMin. AGBD (Mg/ha)\nAvg. AGBD (Mg/ha)\nMax. AGBD (Mg/ha)\n\n\n\n\n1\nRandom Forest (RF)\n21.406\n41.252\n68.559\n\n\n\n\n\nTable 4 Model Assessment Metrics\n\n\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Model\nR¬≤ Training (70%)\nR¬≤ Testing (30%)\nRMSE (Mg/ha) Training\nRMSE (Mg/ha) Testing\n\n\n\n\n1\nRandom Forest (RF)\n0.73\n0.209\n29.637\n27.751\n\n\n\nThese metrics showcase the effectiveness of the Random Forest model in predicting aboveground biomass density, indicating its potential to estimate biomass with considerable accuracy.\n\n\n\nTree Area Calculation\nThe total tree land area within the University of Malaya study area has been calculated:\n// Tree Land area (hectare, ha) in the Study area (University of Malaya)\n// Calculate the tree area in hectares\nvar treeArea = forest_mask.select('Map');\nvar areaImage = treeArea.multiply(ee.Image.pixelArea());\n\n// Now that each pixel for the forest class in the image has the value\n// equal to its area, we can sum up all the values in the region\n// to get the total forested area.\nvar area = areaImage.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: boundary,\n  scale: 30, // Specify the scale that matches your projection\n  maxPixels: 1e9 // Adjust as needed\n});\n\n// Convert the forested area to hectares\nvar treeAreaHectares = ee.Number(area.get('Map')).divide(10000); // 1 square meter = 0.0001 hectares\n\n// Print the tree area in hectares\nprint('Tree Area (Hectares) in the University of Malaya:', treeAreaHectares);\nTree Area (Hectares) in the University of Malaya: 2109.434 hectares\n\n\nCarbon Storage Calculation\nThe carbon storage estimation process involves the computation of aboveground carbon storage based on the calculated aboveground biomass density and the specified tree area in the University of Malaya study area:\n\nAboveground Carbon Storage Calculation\n// Aboveground Carbon Storage Calculation in Study Area (UM)\n// Formula: Aboveground Carbon Storage (Mg, Megagram also known as tonnes) = Aboveground Biomass (Mg/ha) x Carbon Content Factor x Tree Area (ha)\n\n// Define the inputs\nvar minAbovegroundBiomass = 21.406166582107545; // Min aboveground biomass (Mg/ha)\nvar maxAbovegroundBiomass = 68.5589729944865;   // Max aboveground biomass (Mg/ha)\nvar avgAbovegroundBiomass = 41.252308736491585; // Avg aboveground biomass (Mg/ha)\nvar carbonContentFactor = 0.5;                   // Carbon content factor\nvar treeArea = 2109.4340347280445;               // Tree area 2109.4340347280445 ha\n\n// Calculate Aboveground Carbon Storage\nvar abovegroundCarbonStorageMin = minAbovegroundBiomass * carbonContentFactor * treeArea;\nvar abovegroundCarbonStorageMax = maxAbovegroundBiomass * carbonContentFactor * treeArea;\nvar abovegroundCarbonStorageAvg = avgAbovegroundBiomass * carbonContentFactor * treeArea;\n\n// Print the results\nprint('Aboveground Carbon Storage (Mg, tonnes) in the University of Malaya:');\nprint('Minimum Estimate:', abovegroundCarbonStorageMin, 'Mg (tonnes)');\nprint('Maximum Estimate:', abovegroundCarbonStorageMax, 'Mg (tonnes)');\nprint('Average Estimate:', abovegroundCarbonStorageAvg, 'Mg (tonnes)');\nAboveground Carbon Stock Estimation Results\nTable 5 summarizes the estimates for aboveground carbon stock in the University of Malaya study area:\n\n\n\n\n\n\n\n\n\n\nNo\nAboveground Carbon Stock Area\nMin. Carbon (Mg)\nAvg. Carbon (Mg)\nMax. Carbon (Mg)\n\n\n\n\n1\nUniversity of Malaya\n22577.448\n43509.512\n72310.316\n\n\n\nThese estimates indicate the potential carbon storage within the specified study area, providing valuable insights into its carbon sequestration capacity and highlighting its significance in environmental assessments and management strategies."
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#web-application-showcase",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#web-application-showcase",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Web Application Showcase",
    "text": "Web Application Showcase\n\nCanopy Height Prediction Random Forest Model without PCA\nCanopy Height Prediction Random Forest Model with PCA\nAboveground Biomass Prediction Random Forest Model"
  },
  {
    "objectID": "posts/canopy height & carbon stock prediction/canopy-carbon.html#conclusion",
    "href": "posts/canopy height & carbon stock prediction/canopy-carbon.html#conclusion",
    "title": "Canopy Height and Carbon Stock Prediction Using Satellite Images with Machine Learning on GEE",
    "section": "Conclusion",
    "text": "Conclusion\nThis project leveraged satellite imagery and machine learning through Google Earth Engine to estimate canopy height and carbon content in the University of Malaya area. By analyzing diverse datasets like Sentinel-1, Sentinel-2, SRTM, ESA WorldCover, GEDI L2A, and GEDI L4B, we gained valuable insights.\n\nKey Findings\n\nCanopy Height Prediction: Various machine learning models were employed, showcasing the potential to estimate canopy height. Random Forest and RF with Principal Component Analysis emerged as promising models.\nAboveground Biomass Density: The Random Forest model proved valuable in estimating biomass density, providing insights into vegetation health and carbon sequestration potential.\nCarbon Storage Assessment: The estimation of aboveground carbon stocks revealed significant reserves within the University of Malaya area, highlighting its role as a crucial carbon sink.\n\n\n\nSignificance\nThis study sheds light on the University of Malaya‚Äôs ecosystem, emphasizing its importance in carbon sequestration. Understanding its capacity for storing carbon reinforces the significance of preserving such areas for environmental stability and biodiversity conservation.\n\n\nFuture Directions\nThe methodologies employed and findings derived from this research offer implications for environmental assessments and conservation strategies. The accuracy of predictive models and detailed carbon stock estimations could serve as a foundational framework for similar studies globally, aiding ecosystem management and informed decision-making.\nIn summary, this project underscores the University of Malaya‚Äôs ecosystem‚Äôs vital role in carbon sequestration, emphasizing the significance of satellite imagery and machine learning in environmental assessments and conservation endeavors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I‚Äôm Wang Huang Han, a dedicated data science master‚Äôs student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I‚Äôm engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data‚Äîa fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet‚Äôs connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I‚Äôm Wang Huang Han, a dedicated data science master‚Äôs student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I‚Äôm engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data‚Äîa fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet‚Äôs connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Wang Huang Han",
    "section": "",
    "text": "Hello! I‚Äôm Wang Huang Han, a dedicated data science master‚Äôs student at the University of Malaya. My passion lies in building strong connections between people and leveraging data-driven insights for positive impact.\nWith a background in marketing and banking, I bring a unique blend of skills to the table. Over the past year, I have immersed myself in learning data analytics, machine learning, and deep learning techniques through coursework and collaborative projects. Currently, I‚Äôm engaged in research using Google Earth Engine to predict canopy height and carbon stock from remote sensing data‚Äîa fascinating intersection of technology and environmental impact.\nI thrive as a team player and communicator, qualities honed through my experiences. As a final-year student, I am enthusiastic about contributing and growing in my new career role. I aspire to make a meaningful impact on society for a better world.\nLet‚Äôs connect and explore the possibilities of collaboration and innovation together!"
  },
  {
    "objectID": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html",
    "href": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html",
    "title": "Immersing Indian Culture - Garden of Gods - Penang (George Town)",
    "section": "",
    "text": "Malaysia is located at the intersection of the East and West, including Chinese, Islamic, Indian, and Western cultures. If you are interested in ancient culture as I do, Malaysia would be a great fit.\nIn this article, I will share a 2-day (09.12.23-10.12.23) Indian culture forum and Hinduism temple guided tour at George Town, Penang (Malaysia).\nBefore the story starts, I want to give credit to the lecturer, Mr.¬†Tang Ah Chai (Èô≥‰∫ûÊâç), the organizer Penang City Eye ÂüéË¶ñÂ†± & Penang Harmony Corporation, and the participants.\nCultural activities and engagement are always interesting to me. Since childhood, my eyes have been caught by vivid cultural things such as Chinese, Indian, Indonesian, Malaysian, and Islamic cultures. With the background and foundation of Chinese culture, it immensely strengthens my understanding and appreciation of other cultures.\nIn the Chinese ancient classic ‚ÄúI-Ching (The Book of Changes),‚Äù it said, ‚ÄúËßÄ‰πéÂ§©ÊñáÔºå‰ª•ÂØüÊôÇËÆäÔºõËßÄ‰πé‰∫∫ÊñáÔºå‰ª•ÂåñÊàêÂ§©‰∏ã„ÄÇ,‚Äù means that by observing the movement of constellations, we can learn about the change of seasons; by observing development of human culture, we can enlighten the people and build a civilized society."
  },
  {
    "objectID": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html#introduction",
    "href": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html#introduction",
    "title": "Immersing Indian Culture - Garden of Gods - Penang (George Town)",
    "section": "Introduction",
    "text": "Introduction\nMalaysia is a multi-ethnic country, with 60% of the population being Malays (nearly 20 million), almost 22.6% Chinese (around 6 million), and 7% Indians (about 2 million), with 90% of Indian ancestors coming from Tamil Nadu and the rest being aboriginals and other ethnic groups.\nAfter this brief overview of the population proportion in Malaysia, let‚Äôs dive into the topic ‚ÄúImmersing Indian Culture - Garden of Gods.‚Äù\nThis forum schedule is shown below:\n09/12/2023 (Saturday):\n\nForum: Indian Culture and History & Classics Introduction (8:30 - 16:00)\nGuided Tour: Penang Indian Heritage Museum & Penang Nagarathar Sivan Temple (17:00 - 19:30)\n\n10/12/2023 (Sunday):\n\nGuided Tour: Sri Varasithi Vinayagar Temple & Arulmigu Sri Mahamariamman Temple & Nagarathar Kovil Veedu (9:00 - 11:30)\n\n\n\n\n\n\n\n\nLecture‚Äôs location @Penang Harmony Centre\n\n\n\n\n\n\n\nThe opening of Indian culture forum\n\n\n\n\nFigure¬†1: The first day of the forum at Penang Harmony Centre with a brief introduction."
  },
  {
    "objectID": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html#impressive-part",
    "href": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html#impressive-part",
    "title": "Immersing Indian Culture - Garden of Gods - Penang (George Town)",
    "section": "Impressive Part",
    "text": "Impressive Part\n\nCultural Forum\nIndian culture, over 5000 years old and undergoing the test of time, has transformed into a colorful and unique characteristic. Each state in India has its solid and vivid features. When people dive more into it, they will find much more they don‚Äôt know about India. To echo the adage ‚ÄúYou don‚Äôt know what you don‚Äôt know‚Äù found in Indian Market Characteristics by Ravindra Vasisht, let‚Äôs begin with the colorful culture about India.\nAfter learning and participating in these two days of fulfilling courses and guided tours, it was like experiencing an in-depth SPA of Indian culture. From the first day, teacher Tang Ah Chai structurally talked about the origin of Indian culture (the origin of civilization, the caste system, etc.) to wonderful classics and epics (Mahabharata, Ramayana). Then, we can find that the entire Southeast Asia and East Asia are deeply influenced by Indian cultures, such as the shadow puppet show Wayang kulit in Malaysia and Indonesia, the Ramakien in Thailand, and the well-known novel Journey to the West (Ë•øÈÅäË®ò) in Chinese culture.\nIn addition, the Vedic classics, the Puranas, the Upanishads (the unification of Atman and Brahman), and the Aranyaka (like the Indian version of career planning), teacher Tang using a life-oriented narrative style, which makes me interested in finding time to dig these Hindu classics gradually.\n\n\nGuided Tour\nThe most exciting part is the guided tour of the Indian temples and museum. At this time, ‚Äúthe Chettiar ethnic group,‚Äù who came to Malaysia from India and mainly engaged in traders, weaving, agricultural, and land-owning businesses, came to my mind deeply.\n\n\n\n\n\n\n\nMore than 20 lunch boxes @Penang Indian Heritage Museum\n\n\n\n\n\n\n\nMoney lender business by Chettiar group\n\n\n\n\nFigure¬†2: Chettiar Group‚Äôs big family and their business at Penang Indian Heritage Museum.\n\n\nNot only did I visit the Penang Indian Heritage Museum to understand the united force of the Chettiar family, but I also saw as many as 20 lunch boxes taller than ordinary people, which was enough to show the enormous family during that time.\n\n\n\n\n\n\n\nKumkuma is a powder used for social and religious markings. @Arulmigu Sri Mahamariamman Temple\n\n\n\n\n\n\n\nColorful Statue (musician god in heaven) on the Hindu temple roof @Penang Nagarathar Sivan Temple\n\n\n\n\nFigure¬†3: The religious things and statues are usually seen in Tamil Nadu-style temples.\n\n\nDifferent color powders have different meanings:\n\nWhite: The most universal color symbolizing liberation and freedom from reincarnation reminds everyone that people will eventually be reborn and burned into ashes after cremation, just like this white ashes.\nRed: The red dot symbolizing love and bravery not only represents married women, but its symbolic meaning is to protect women and their husbands from harm and destruction of their intimate relationships from the evil eye.\nYellow: The yellow dot made from turmeric powder can be applied on the forehead and the whole body. It has a cooling effect and can calm the mind. The yellow dot also symbolizes wealth.\nHoly Water: Priests will give prayers after pooja/puja (offering) to symbolize blessing and bring fortune. Normally, prayers use their right hand to hold and drink holy water.\n\n\n\n\n\n\n\n\nExplained the holding symbols (cane) of the beauty, love, and abundance goddess (Tripura Sundari) @Arulmigu Sri Mahamariamman Temple\n\n\n\n\n\n\n\nExplained the vahana (god‚Äôs vehicle) called Nandi (sacred bull) with Lord Shiva (Linga) @Arulmigu Sri Mahamariamman Temple\n\n\n\n\nFigure¬†4: Teacher Tang explained the intricate parts of the Hinduism temple in detail.\n\n\n\n\n\n\n\n\n\nNagarathar Kovil Veedu means Chettiar‚Äôs temple, which worships the main god lord Murugan.\n\n\n\n\n\n\n\nInside Chettiar‚Äôs private temple, listened to their story about the Chettiar group from India to Malaysia. In the middle, sitting on a chair, they are teacher Tang and manager Mr.¬†Shiva separately.\n\n\n\n\nFigure¬†5: Visiting Nagarathar Kovil Veedu to understand the history and process of the Chettiar people.\n\n\nThe most impressive thing was the visit to Nagarathar Kovil Veedu on the second day, which allowed us to communicate directly with Mr.¬†Shiva, the person in charge of Penang Nagarathar Sivan Temple. He mentioned that their ancestors came to Malaysia with the golden Vel (spear) of lord Murugan. They always remember that the wealth earned is ultimately used to worship the gods. From a few minutes of interviews, I understand that Chettiar is good at doing business and providing loans, and is well-known for their honesty and trust.\nAlthough the Chettiar family is no longer engaged in business lending as the primary industry in the past, it is scattered in various fields. Some are lawyers, doctors, software and hardware engineers, professors (teachers), etc. You can feel that Chettiar people are proud of their identity and reveal some confidence and pride.\nNowadays, they added that there are still people who come to them for loans, some to pay tuition or to buy a house. The symbolic meaning is greater than the substantive meaning, hoping to bring them good luck, such as the Chinese ‚ÄúThe arrival of good fortune (bat Á¶èÂà∞)‚Äù are also the most exciting parts of the conversation.\n\n\n\n\n\n\n\nGroup Picture taken in front of Arulmigu Sri Mahamariamman Temple\n\n\n\n\n\n\n\nSilver Chariot owned by Nagarathar temples mainly used in the Thaipusam festival\n\n\n\n\nFigure¬†6: Both the Hindu temple architecture and the chariot‚Äôs decoration are intricate with many details."
  },
  {
    "objectID": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html#key-takeaways",
    "href": "culture/Indian culture_Garden of the Gods_Penang(12.2023)/begining.html#key-takeaways",
    "title": "Immersing Indian Culture - Garden of Gods - Penang (George Town)",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe more you know about India, the more you don‚Äôt know!\nIt is worth participating in this cultural activity with teacher Tang Ah Chai‚Äôs detailed explanation, which makes the intricate Indian culture easy to understand and accessible to everyone of all ages.\n\nTo conclude, through this 2-day Indian cultural activity (Garden of Gods), I realized how possible it is to break the race barriers between others. The key is to understand different cultures and respect each other.\nIn the end, all of us were blessed and embarked on the next journey in our life.\n\n\n\n\n\n\n\n\nI was interviewed by the Taiwan Public Television Service (PTS) at the end of the cultural guided tour on 10.12.2023.\n\n\n\n\n\n\n\nGetting a blessing from the priest and lord Murugan\n\n\n\n\nFigure¬†7: Click ‚ÄúPenang Harmony Corporation promotes inter-ethnic guided tours and visits religious sites in hopes of breaking stereotypes‚Äù to read the report from PTS news."
  }
]